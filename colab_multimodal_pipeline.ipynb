{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ Multimodal Donor Legacy Intent Prediction\n",
        "\n",
        "This notebook runs the complete multimodal fusion pipeline combining:\n",
        "- **Tabular Data**: Donor demographics and giving history (9 features)\n",
        "- **Text Data**: Contact reports via BERT embeddings (768-dim)\n",
        "- **Graph Data**: Family relationships via GNN embeddings (64-dim)\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Quick Start Guide\n",
        "\n",
        "### **Step 1: Set Up GPU Runtime**\n",
        "1. Click **Runtime** ‚Üí **Change runtime type**\n",
        "2. Select **Hardware accelerator**: **GPU** (Tesla T4 recommended)\n",
        "3. Click **Save**\n",
        "\n",
        "### **Step 2: Run Setup Cell**\n",
        "- Run Cell 2 to install all required packages (~2 minutes)\n",
        "\n",
        "### **Step 3: Upload Files When Prompted**\n",
        "\n",
        "You'll need to upload these files:\n",
        "\n",
        "#### **üìä Dataset Files** (Cell 5):\n",
        "- `donors.csv` (50,000 donors)\n",
        "- `contact_reports.csv` (32,665 reports)\n",
        "- `relationships.csv` (15,000 relationships)\n",
        "\n",
        "#### **ü§ñ Pre-trained Model** (Cell 5 - Optional):\n",
        "- `best_contact_classifier.pt` (trained BERT model)\n",
        "  - If not uploaded, will train new model (~10 min)\n",
        "\n",
        "#### **üíª Source Code Files** (Cell 7):\n",
        "- `multimodal_arch.py`\n",
        "- `bert_pipeline.py`\n",
        "- All files from `gnn_models/` folder\n",
        "- All files from `data_generation/` folder\n",
        "\n",
        "### **Step 4: Run All Cells**\n",
        "- Click **Runtime** ‚Üí **Run all**\n",
        "- Total time: ~5-10 minutes with pre-trained model\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Success Indicators\n",
        "\n",
        "You should see:\n",
        "- ‚úÖ `‚úÖ Imported GNN modules successfully`\n",
        "- ‚úÖ `Loaded from checkpoint['model_state_dict']`\n",
        "- ‚úÖ `‚úÖ Saved tabular feature scaler`\n",
        "- ‚úÖ No \"Falling back to dummy embeddings\" messages\n",
        "- ‚úÖ Train/Val Loss showing numbers (not `nan`)\n",
        "- ‚úÖ Both classes predicted in classification report\n",
        "\n",
        "---\n",
        "\n",
        "## üìà Expected Results\n",
        "\n",
        "- **Test Accuracy**: 75-82%\n",
        "- **Test AUC**: 0.75-0.85\n",
        "- **Macro F1 Score**: 0.60-0.70\n",
        "- **Legacy Intent Recall**: 40-50% (minority class detected!)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ùì Need Help?\n",
        "\n",
        "- **Detailed Guide**: See `COLAB_QUICKSTART_GUIDE.md`\n",
        "- **Complete Documentation**: See `FINAL_FIX_SUMMARY.md`\n",
        "- **Project Structure**: See `PROJECT_STRUCTURE.md`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup & Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install transformers datasets accelerate\n",
        "!pip install torch-geometric\n",
        "!pip install scikit-learn matplotlib seaborn\n",
        "!pip install pandas numpy tqdm\n",
        "!pip install networkx\n",
        "\n",
        "print(\"‚úÖ All packages installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "# Set up paths\n",
        "os.makedirs('src', exist_ok=True)\n",
        "os.makedirs('synthetic_donor_dataset', exist_ok=True)\n",
        "\n",
        "# Add src to Python path\n",
        "sys.path.append('src')\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üöÄ Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "print(\"‚úÖ Environment setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Upload Dataset Files\n",
        "\n",
        "Upload the following files from your project:\n",
        "- `donors.csv`\n",
        "- `contact_reports.csv` \n",
        "- `relationships.csv`\n",
        "- `best_contact_classifier.pt` (optional - will train new model if not provided)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "print(\"üìÅ Please upload your dataset files:\")\n",
        "print(\"1. donors.csv\")\n",
        "print(\"2. contact_reports.csv\")\n",
        "print(\"3. relationships.csv\")\n",
        "print(\"4. best_contact_classifier.pt (optional)\")\n",
        "print()\n",
        "\n",
        "# Upload files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move files to correct directories\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.csv'):\n",
        "        shutil.move(filename, f'synthetic_donor_dataset/{filename}')\n",
        "        print(f\"‚úÖ Moved {filename} to synthetic_donor_dataset/\")\n",
        "    elif filename.endswith('.pt'):\n",
        "        shutil.move(filename, f'{filename}')\n",
        "        print(f\"‚úÖ Moved {filename} to root directory\")\n",
        "\n",
        "print(\"\\nüìä Checking uploaded files:\")\n",
        "for file in ['synthetic_donor_dataset/donors.csv', 'synthetic_donor_dataset/contact_reports.csv', 'synthetic_donor_dataset/relationships.csv']:\n",
        "    if os.path.exists(file):\n",
        "        df = pd.read_csv(file)\n",
        "        print(f\"‚úÖ {file}: {len(df):,} rows\")\n",
        "    else:\n",
        "        print(f\"‚ùå {file}: Not found\")\n",
        "\n",
        "if os.path.exists('best_contact_classifier.pt'):\n",
        "    print(\"‚úÖ best_contact_classifier.pt: Found (will use for BERT embeddings)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è best_contact_classifier.pt: Not found (will train new BERT model)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Upload Project Code Files\n",
        "\n",
        "Upload the following Python files from your `src/` directory:\n",
        "- `multimodal_arch.py`\n",
        "- `bert_pipeline.py`\n",
        "- `gnn_models/` folder (all files)\n",
        "- `data_generation/` folder (all files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìÅ Please upload your project code files:\")\n",
        "print(\"1. multimodal_arch.py\")\n",
        "print(\"2. bert_pipeline.py\")\n",
        "print(\"3. All files from gnn_models/ folder\")\n",
        "print(\"4. All files from data_generation/ folder\")\n",
        "print()\n",
        "\n",
        "# Upload code files\n",
        "uploaded_code = files.upload()\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs('src/gnn_models', exist_ok=True)\n",
        "os.makedirs('src/data_generation', exist_ok=True)\n",
        "\n",
        "# Move files to correct locations\n",
        "for filename in uploaded_code.keys():\n",
        "    if 'gnn_models' in filename:\n",
        "        shutil.move(filename, f'src/{filename}')\n",
        "        print(f\"‚úÖ Moved {filename} to src/gnn_models/\")\n",
        "    elif 'data_generation' in filename:\n",
        "        shutil.move(filename, f'src/{filename}')\n",
        "        print(f\"‚úÖ Moved {filename} to src/data_generation/\")\n",
        "    elif filename.endswith('.py') and 'src' in filename:\n",
        "        # Handle files like 'src/multimodal_arch.py'\n",
        "        new_name = filename.replace('src/', '')\n",
        "        shutil.move(filename, f'src/{new_name}')\n",
        "        print(f\"‚úÖ Moved {filename} to src/{new_name}\")\n",
        "    elif filename.endswith('.py'):\n",
        "        shutil.move(filename, f'src/{filename}')\n",
        "        print(f\"‚úÖ Moved {filename} to src/\")\n",
        "\n",
        "print(\"\\nüìä Checking uploaded code files:\")\n",
        "code_files = [\n",
        "    'src/multimodal_arch.py',\n",
        "    'src/bert_pipeline.py',\n",
        "    'src/gnn_models/gnn_pipeline.py',\n",
        "    'src/gnn_models/gnn_models.py',\n",
        "    'src/data_generation/data_generation.py'\n",
        "]\n",
        "\n",
        "for file in code_files:\n",
        "    if os.path.exists(file):\n",
        "        print(f\"‚úÖ {file}\")\n",
        "    else:\n",
        "        print(f\"‚ùå {file}: Not found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create Multimodal Pipeline Script\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the multimodal pipeline script for Colab\n",
        "print(\"Creating multimodal pipeline script...\")\n",
        "\n",
        "# Write the pipeline script directly\n",
        "with open('colab_multimodal_pipeline.py', 'w') as f:\n",
        "    f.write('''#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Multimodal Pipeline for Google Colab\n",
        "Combines BERT, GNN, and tabular data for donor legacy intent prediction\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src directory to path for imports\n",
        "sys.path.append('src')\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Load the synthetic donor dataset\"\"\"\n",
        "    print(\"Loading synthetic donor dataset...\")\n",
        "    \n",
        "    # Load donors data\n",
        "    donors_df = pd.read_csv('synthetic_donor_dataset/donors.csv')\n",
        "    print(f\"Loaded {len(donors_df):,} donors\")\n",
        "    \n",
        "    # Load contact reports\n",
        "    contact_reports_df = pd.read_csv('synthetic_donor_dataset/contact_reports.csv')\n",
        "    print(f\"Loaded {len(contact_reports_df):,} contact reports\")\n",
        "    \n",
        "    return donors_df, contact_reports_df\n",
        "\n",
        "def load_bert_embeddings(donors_df, contact_reports_df):\n",
        "    \"\"\"Load or generate BERT embeddings from the trained model\"\"\"\n",
        "    print(\"Loading BERT embeddings...\")\n",
        "    \n",
        "    try:\n",
        "        # Import BERT pipeline components\n",
        "        from bert_pipeline import (\n",
        "            setup_transformer_environment, \n",
        "            select_model, \n",
        "            EmbeddingExtractor,\n",
        "            run_bert_pipeline_on_contact_reports\n",
        "        )\n",
        "        \n",
        "        # Check if we have a trained model\n",
        "        model_path = 'best_contact_classifier.pt'\n",
        "        if os.path.exists(model_path):\n",
        "            print(f\"Found trained model: {model_path}\")\n",
        "            \n",
        "            # Load the trained model\n",
        "            device = setup_transformer_environment()\n",
        "            model_info = select_model('bert')  # Use BERT as default\n",
        "            \n",
        "            # Load model state (handle different checkpoint formats)\n",
        "            checkpoint = torch.load(model_path, map_location=device)\n",
        "            model = model_info['model']\n",
        "            \n",
        "            # Try different checkpoint formats\n",
        "            try:\n",
        "                if isinstance(checkpoint, dict):\n",
        "                    if 'model_state_dict' in checkpoint:\n",
        "                        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "                        print(\"Loaded from checkpoint['model_state_dict']\")\n",
        "                    elif 'state_dict' in checkpoint:\n",
        "                        model.load_state_dict(checkpoint['state_dict'])\n",
        "                        print(\"Loaded from checkpoint['state_dict']\")\n",
        "                    else:\n",
        "                        # Checkpoint dict might contain the state dict directly\n",
        "                        model.load_state_dict(checkpoint)\n",
        "                        print(\"Loaded checkpoint as state dict\")\n",
        "                else:\n",
        "                    # Checkpoint is the state dict itself\n",
        "                    model.load_state_dict(checkpoint)\n",
        "                    print(\"Loaded checkpoint directly\")\n",
        "            except Exception as load_error:\n",
        "                print(f\"Error loading checkpoint: {load_error}\")\n",
        "                print(f\"Checkpoint type: {type(checkpoint)}\")\n",
        "                if isinstance(checkpoint, dict):\n",
        "                    print(f\"Checkpoint keys: {checkpoint.keys()}\")\n",
        "                raise\n",
        "            \n",
        "            model.to(device)\n",
        "            \n",
        "            # Initialize tokenizer\n",
        "            tokenizer = model_info['tokenizer']\n",
        "            \n",
        "            # Create embedding extractor\n",
        "            extractor = EmbeddingExtractor(model, tokenizer, device)\n",
        "            \n",
        "            # Get contact report texts for donors\n",
        "            donor_texts = []\n",
        "            for donor_id in donors_df['ID']:\n",
        "                # Get all contact reports for this donor\n",
        "                donor_reports = contact_reports_df[contact_reports_df['Donor_ID'] == donor_id]\n",
        "                \n",
        "                if len(donor_reports) > 0:\n",
        "                    # Combine all report texts for this donor\n",
        "                    combined_text = ' '.join(donor_reports['Report_Text'].fillna('').astype(str))\n",
        "                    donor_texts.append(combined_text)\n",
        "                else:\n",
        "                    # Use empty text if no contact reports\n",
        "                    donor_texts.append('')\n",
        "            \n",
        "            # Extract embeddings\n",
        "            bert_embeddings = extractor.extract_embeddings(donor_texts, batch_size=16)\n",
        "            \n",
        "            print(f\"BERT embeddings shape: {bert_embeddings.shape}\")\n",
        "            return bert_embeddings\n",
        "            \n",
        "        else:\n",
        "            print(\"No trained BERT model found. Training new model...\")\n",
        "            \n",
        "            # Run the full BERT pipeline to train and extract embeddings\n",
        "            bert_results = run_bert_pipeline_on_contact_reports(\n",
        "                data_dir=\"synthetic_donor_dataset\",\n",
        "                model_choice='bert',\n",
        "                batch_size=16,\n",
        "                epochs=3,  # Reduced for faster training\n",
        "                learning_rate=2e-5\n",
        "            )\n",
        "            \n",
        "            # Extract embeddings using the trained model\n",
        "            extractor = bert_results['extractor']\n",
        "            \n",
        "            # Get contact report texts for donors\n",
        "            donor_texts = []\n",
        "            for donor_id in donors_df['ID']:\n",
        "                donor_reports = contact_reports_df[contact_reports_df['Donor_ID'] == donor_id]\n",
        "                if len(donor_reports) > 0:\n",
        "                    combined_text = ' '.join(donor_reports['Report_Text'].fillna('').astype(str))\n",
        "                    donor_texts.append(combined_text)\n",
        "                else:\n",
        "                    donor_texts.append('')\n",
        "            \n",
        "            bert_embeddings = extractor.extract_embeddings(donor_texts, batch_size=16)\n",
        "            print(f\"BERT embeddings shape: {bert_embeddings.shape}\")\n",
        "            return bert_embeddings\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading BERT embeddings: {e}\")\n",
        "        import traceback\n",
        "        print(\"Full error traceback:\")\n",
        "        traceback.print_exc()\n",
        "        print(\"\\n‚ö†Ô∏è  Falling back to dummy embeddings (random noise)...\")\n",
        "        print(\"WARNING: Model will not use real text features!\")\n",
        "        # Fallback to dummy embeddings\n",
        "        rng = np.random.default_rng(42)\n",
        "        bert_embeddings = rng.standard_normal((len(donors_df), 768)).astype(np.float32)\n",
        "        print(f\"BERT embeddings shape: {bert_embeddings.shape}\")\n",
        "        return bert_embeddings\n",
        "\n",
        "def load_gnn_embeddings(donors_df, relationships_df):\n",
        "    \"\"\"Load or generate GNN embeddings from the trained model\"\"\"\n",
        "    print(\"Loading GNN embeddings...\")\n",
        "    \n",
        "    try:\n",
        "        # Import GNN pipeline components\n",
        "        import sys\n",
        "        if 'src' not in sys.path:\n",
        "            sys.path.append('src')\n",
        "        \n",
        "        # Try importing - handle different module structures\n",
        "        try:\n",
        "            from gnn_models.gnn_pipeline import main_gnn_pipeline\n",
        "            from gnn_models.gnn_analysis import get_node_embeddings\n",
        "            print(\"‚úÖ Imported GNN modules successfully\")\n",
        "        except (ImportError, ModuleNotFoundError) as import_error:\n",
        "            print(f\"Warning: {import_error}\")\n",
        "            print(\"Trying alternative import...\")\n",
        "            import gnn_models.gnn_pipeline as gnn_pipeline_module\n",
        "            import gnn_models.gnn_analysis as gnn_analysis_module\n",
        "            main_gnn_pipeline = gnn_pipeline_module.main_gnn_pipeline\n",
        "            get_node_embeddings = gnn_analysis_module.get_node_embeddings\n",
        "            print(\"‚úÖ Imported GNN modules via alternative path\")\n",
        "        \n",
        "        # Check if we have relationships data\n",
        "        if relationships_df.empty or len(relationships_df) == 0:\n",
        "            print(\"No relationship data found. Using dummy GNN embeddings...\")\n",
        "            rng = np.random.default_rng(42)\n",
        "            gnn_embeddings = rng.standard_normal((len(donors_df), 64)).astype(np.float32)\n",
        "            print(f\"GNN embeddings shape: {gnn_embeddings.shape}\")\n",
        "            return gnn_embeddings\n",
        "        \n",
        "        # Run GNN pipeline to get embeddings\n",
        "        print(\"Running GNN pipeline to generate embeddings...\")\n",
        "        gnn_results = main_gnn_pipeline(\n",
        "            donors_df=donors_df,\n",
        "            relationships_df=relationships_df,\n",
        "            contact_reports_df=None,  # Not needed for GNN\n",
        "            giving_history_df=None    # Not needed for GNN\n",
        "        )\n",
        "        \n",
        "        # Extract embeddings\n",
        "        gnn_embeddings = gnn_results['embeddings']\n",
        "        print(f\"GNN embeddings shape: {gnn_embeddings.shape}\")\n",
        "        \n",
        "        # Ensure embeddings match donor count\n",
        "        if len(gnn_embeddings) != len(donors_df):\n",
        "            print(f\"Warning: GNN embeddings count ({len(gnn_embeddings)}) doesn't match donor count ({len(donors_df)})\")\n",
        "            # Pad or truncate as needed\n",
        "            if len(gnn_embeddings) < len(donors_df):\n",
        "                # Pad with zeros\n",
        "                padding = np.zeros((len(donors_df) - len(gnn_embeddings), gnn_embeddings.shape[1]))\n",
        "                gnn_embeddings = np.vstack([gnn_embeddings, padding])\n",
        "            else:\n",
        "                # Truncate\n",
        "                gnn_embeddings = gnn_embeddings[:len(donors_df)]\n",
        "        \n",
        "        return gnn_embeddings\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading GNN embeddings: {e}\")\n",
        "        import traceback\n",
        "        print(\"Full error traceback:\")\n",
        "        traceback.print_exc()\n",
        "        print(\"\\n‚ö†Ô∏è  Falling back to dummy embeddings (random noise)...\")\n",
        "        print(\"WARNING: Model will not use real graph features!\")\n",
        "        # Fallback to dummy embeddings\n",
        "        rng = np.random.default_rng(42)\n",
        "        gnn_embeddings = rng.standard_normal((len(donors_df), 64)).astype(np.float32)\n",
        "        print(f\"GNN embeddings shape: {gnn_embeddings.shape}\")\n",
        "        return gnn_embeddings\n",
        "\n",
        "def load_actual_embeddings(donors_df, contact_reports_df):\n",
        "    \"\"\"Load actual embeddings from BERT and GNN pipelines\"\"\"\n",
        "    print(\"Loading actual embeddings from trained models...\")\n",
        "    \n",
        "    # Load relationships data for GNN\n",
        "    relationships_df = pd.read_csv('synthetic_donor_dataset/relationships.csv')\n",
        "    \n",
        "    # Load BERT embeddings\n",
        "    bert_embeddings = load_bert_embeddings(donors_df, contact_reports_df)\n",
        "    \n",
        "    # Load GNN embeddings  \n",
        "    gnn_embeddings = load_gnn_embeddings(donors_df, relationships_df)\n",
        "    \n",
        "    return bert_embeddings, gnn_embeddings\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"MULTIMODAL DONOR LEGACY INTENT PREDICTION\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Load data\n",
        "    donors_df, contact_reports_df = load_data()\n",
        "    \n",
        "    # Load actual embeddings from trained models\n",
        "    bert_embeddings, gnn_embeddings = load_actual_embeddings(donors_df, contact_reports_df)\n",
        "    \n",
        "    # Import multimodal architecture\n",
        "    from multimodal_arch import run_multimodal_fusion_pipeline\n",
        "    \n",
        "    # Run the multimodal fusion pipeline\n",
        "    print(\"\\\\n\" + \"=\" * 60)\n",
        "    print(\"STARTING MULTIMODAL FUSION PIPELINE\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    results = run_multimodal_fusion_pipeline(\n",
        "        donors_df=donors_df,\n",
        "        contact_reports_df=contact_reports_df,\n",
        "        bert_embeddings=bert_embeddings,  # From BERT pipeline\n",
        "        gnn_embeddings=gnn_embeddings,     # From GNN pipeline\n",
        "        target_column='Legacy_Intent_Binary',\n",
        "        batch_size=32,\n",
        "        epochs=50,\n",
        "        learning_rate=1e-3\n",
        "    )\n",
        "    \n",
        "    # Access results\n",
        "    test_accuracy = results['results']['test_accuracy']\n",
        "    test_auc = results['results']['test_auc']\n",
        "    \n",
        "    print(\"\\\\n\" + \"=\" * 60)\n",
        "    print(\"FINAL RESULTS SUMMARY\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Test AUC: {test_auc:.4f}\" if test_auc else \"Test AUC: N/A\")\n",
        "    print(\"Model saved as: multimodal_fusion_model.pt\")\n",
        "    print(\"Best model saved as: best_multimodal_model.pt\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "''')\n",
        "\n",
        "print(\"‚úÖ Created colab_multimodal_pipeline.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Run the Multimodal Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fix the column issue before running the pipeline\n",
        "print(\"üîß Applying column fix...\")\n",
        "\n",
        "# Update the multimodal_arch.py file to handle missing columns\n",
        "fix_code = '''\n",
        "# Fix for missing columns in multimodal_arch.py\n",
        "import os\n",
        "\n",
        "# Read the current file\n",
        "with open('src/multimodal_arch.py', 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Replace the problematic section\n",
        "old_section = \"\"\"    # 1. Tabular features\n",
        "    tabular_cols = [\n",
        "        'Lifetime_Giving', 'Last_Gift', 'Consecutive_Yr_Giving_Count',\n",
        "        'Total_Yr_Giving_Count', 'Engagement_Score', 'Legacy_Intent_Probability',\n",
        "        'Estimated_Age'\n",
        "    ]\"\"\"\n",
        "\n",
        "new_section = \"\"\"    # 1. Tabular features (using available columns)\n",
        "    tabular_cols = [\n",
        "        'Lifetime_Giving', 'Engagement_Score', 'Estimated_Age'\n",
        "    ]\n",
        "    \n",
        "    # Check which columns are available and add them\n",
        "    available_cols = []\n",
        "    for col in tabular_cols:\n",
        "        if col in donors_df.columns:\n",
        "            available_cols.append(col)\n",
        "        else:\n",
        "            print(f\"Warning: Column '{col}' not found in dataset\")\n",
        "    \n",
        "    # Add any additional numeric columns that might be useful\n",
        "    numeric_cols = donors_df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    for col in numeric_cols:\n",
        "        if col not in available_cols and col != 'ID' and col != 'Legacy_Intent_Binary':\n",
        "            available_cols.append(col)\n",
        "    \n",
        "    print(f\"Using tabular columns: {available_cols}\")\n",
        "    tabular_cols = available_cols\"\"\"\n",
        "\n",
        "# Apply the fix\n",
        "if old_section in content:\n",
        "    content = content.replace(old_section, new_section)\n",
        "    with open('src/multimodal_arch.py', 'w') as f:\n",
        "        f.write(content)\n",
        "    print(\"‚úÖ Applied column fix to multimodal_arch.py\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Fix already applied or section not found\")\n",
        "'''\n",
        "\n",
        "exec(fix_code)\n",
        "\n",
        "# Run the complete multimodal pipeline\n",
        "print(\"üöÄ Starting multimodal pipeline...\")\n",
        "exec(open('colab_multimodal_pipeline.py').read())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Download Results\n",
        "\n",
        "Download the trained models and results for use outside of Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download trained models and results\n",
        "files_to_download = [\n",
        "    'multimodal_fusion_model.pt',\n",
        "    'best_multimodal_model.pt',\n",
        "    'best_contact_classifier.pt'  # If retrained\n",
        "]\n",
        "\n",
        "print(\"üì• Downloading trained models:\")\n",
        "for file in files_to_download:\n",
        "    if os.path.exists(file):\n",
        "        files.download(file)\n",
        "        print(f\"‚úÖ Downloaded {file}\")\n",
        "    else:\n",
        "        print(f\"‚ùå {file} not found\")\n",
        "\n",
        "print(\"\\nüéâ Pipeline execution complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Troubleshooting\n",
        "\n",
        "### Common Issues:\n",
        "\n",
        "1. **Import Errors**: Make sure all Python files are uploaded to the correct directories\n",
        "2. **Memory Issues**: Reduce batch size or use fewer epochs\n",
        "3. **CUDA Errors**: The code will automatically fall back to CPU if GPU issues occur\n",
        "4. **Missing Files**: Ensure all dataset files are uploaded\n",
        "\n",
        "### Performance Tips:\n",
        "- Use GPU runtime for faster training\n",
        "- Reduce epochs for quicker testing\n",
        "- Use smaller batch sizes if memory is limited\n",
        "\n",
        "### Support:\n",
        "If you encounter issues, check:\n",
        "- All required files are uploaded\n",
        "- Python packages are installed correctly\n",
        "- Dataset files are in the correct format\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
