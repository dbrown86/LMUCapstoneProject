#!/usr/bin/env python3
"""
WORKING Multimodal Training - NO NaN Issues
Solution: Remove GNN (causing instability), keep Tabular + LSTM + Attention
This version ACTUALLY WORKS without gradient explosion
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
from sklearn.preprocessing import RobustScaler
from sklearn.utils.class_weight import compute_class_weight
from pathlib import Path
from typing import Dict, Optional
import warnings
warnings.filterwarnings('ignore')


class WorkingDataset(Dataset):
    """Working dataset without graph complications"""
    
    def __init__(self, donor_ids, donors_df, giving_df, labels, 
                 scaler=None, fit_scaler=False, max_seq_len=20):
        
        self.donor_ids = donor_ids
        self.labels = torch.tensor(labels, dtype=torch.long)
        self.scaler = scaler
        self.max_seq_len = max_seq_len
        
        # Filter donors
        self.donors_df = donors_df[donors_df['ID'].isin(donor_ids)].copy()
        self.donors_df = self.donors_df.set_index('ID')
        
        # Prepare features
        self.features = self._prepare_features(fit_scaler)
        
        # Index giving history
        self.giving_by_donor = {}
        if giving_df is not None and len(giving_df) > 0:
            print("   üìä Indexing giving history...")
            for did in donor_ids:
                donor_giving = giving_df[giving_df['Donor_ID'] == did]
                if len(donor_giving) > 0:
                    self.giving_by_donor[did] = donor_giving.copy()
    
    def _prepare_features(self, fit_scaler):
        """Prepare features with robust handling"""
        
        feature_cols = {
            'Lifetime_Giving': 'log',
            'Last_Gift': 'log',
            'Total_Yr_Giving_Count': 'standard',
            'Consecutive_Yr_Giving_Count': 'standard',
            'Engagement_Score': 'standard',
            'Legacy_Intent_Probability': 'standard',
            'Legacy_Intent_Binary': 'standard',
            'Estimated_Age': 'standard'
        }
        
        feature_list = []
        feature_names = []
        
        for col, transform in feature_cols.items():
            if col not in self.donors_df.columns:
                continue
            
            data = self.donors_df[col].copy()
            data = data.fillna(data.median() if not data.isna().all() else 0)
            
            if transform == 'log':
                data = np.log1p(np.clip(data, 0, None))
            
            feature_list.append(data.values.reshape(-1, 1))
            feature_names.append(col)
        
        if len(feature_list) == 0:
            raise ValueError("No features available")
        
        features = np.hstack(feature_list)
        
        print(f"      Using {len(feature_names)} features: {feature_names}")
        
        # Robust scaling
        if fit_scaler:
            self.scaler = RobustScaler()
            features = self.scaler.fit_transform(features)
        elif self.scaler:
            features = self.scaler.transform(features)
        
        # Clip and clean
        features = np.clip(features, -5, 5)
        features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
        
        return torch.tensor(features, dtype=torch.float32)
    
    def _get_sequence(self, donor_id):
        """Get giving sequence"""
        if donor_id not in self.giving_by_donor:
            return torch.zeros(self.max_seq_len, 3), 0
        
        donor_giving = self.giving_by_donor[donor_id]
        if 'Gift_Date' in donor_giving.columns:
            donor_giving = donor_giving.sort_values('Gift_Date')
        
        features = []
        for _, row in donor_giving.iterrows():
            amount = np.log1p(max(0, row.get('Gift_Amount', 0)))
            year = row.get('Campaign_Year', 2020) / 2020.0
            anon = 1 if row.get('Anonymous', False) else 0
            features.append([amount, year, anon])
        
        if len(features) == 0:
            return torch.zeros(self.max_seq_len, 3), 0
        
        seq = torch.tensor(features, dtype=torch.float32)
        actual_len = min(len(seq), self.max_seq_len)
        
        if len(seq) < self.max_seq_len:
            padding = torch.zeros(self.max_seq_len - len(seq), 3)
            seq = torch.cat([seq, padding], dim=0)
        else:
            seq = seq[-self.max_seq_len:]
        
        return seq, actual_len
    
    def __len__(self):
        return len(self.donor_ids)
    
    def __getitem__(self, idx):
        donor_id = self.donor_ids[idx]
        features = self.features[idx]
        label = self.labels[idx]
        sequence, seq_len = self._get_sequence(donor_id)
        
        return {
            'features': features,
            'sequence': sequence,
            'seq_len': seq_len,
            'label': label
        }


class WorkingMultimodalModel(nn.Module):
    """Working model: Tabular + LSTM + Attention (NO GNN)"""
    
    def __init__(self, input_dim, hidden_dim=64, num_classes=2, dropout=0.3):
        super().__init__()
        
        # Tabular encoder
        self.tab_enc = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU()
        )
        
        # LSTM for sequences
        self.lstm = nn.LSTM(3, hidden_dim, num_layers=1, batch_first=True)
        
        # Attention
        self.attention = nn.Linear(hidden_dim * 2, 2)
        
        # Fusion
        self.fusion = nn.Sequential(
            nn.Linear(hidden_dim * 2, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # Classifier
        self.classifier = nn.Linear(64, num_classes)
        
        # Initialize
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight, gain=0.5)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.LSTM):
            for name, param in module.named_parameters():
                if 'weight' in name:
                    nn.init.orthogonal_(param, gain=0.5)
                elif 'bias' in name:
                    nn.init.zeros_(param)
    
    def forward(self, features, sequences):
        # Tabular
        tab_out = self.tab_enc(features)
        
        # LSTM
        _, (hidden, _) = self.lstm(sequences)
        seq_out = hidden[-1]
        
        # Combine
        combined = torch.cat([tab_out, seq_out], dim=1)
        
        # Attention
        attn_weights = F.softmax(self.attention(combined), dim=1)
        
        # Fusion
        fused = self.fusion(combined)
        logits = self.classifier(fused)
        
        return logits, attn_weights


def collate_fn(batch):
    return {
        'features': torch.stack([b['features'] for b in batch]),
        'sequences': torch.stack([b['sequence'] for b in batch]),
        'labels': torch.stack([b['label'] for b in batch])
    }


def train_working_multimodal(sample_size=5000, num_epochs=50, batch_size=64):
    """Train working multimodal model (Tabular + LSTM, NO GNN)"""
    
    print("üéØ WORKING MULTIMODAL FUSION TRAINING (Tabular + LSTM)")
    print("=" * 60)
    print(f"Sample: {sample_size:,}, Epochs: {num_epochs}, Batch: {batch_size}")
    
    # Load data
    parquet_dir = Path("data/parquet_export")
    donors_df = pd.read_parquet(parquet_dir / 'donors_with_features.parquet')
    giving_df = pd.read_parquet(parquet_dir / 'giving_history.parquet')
    
    print(f"\nüìÇ Loaded {len(donors_df):,} donors")
    
    # Sample
    if sample_size and sample_size < len(donors_df):
        donors_df = donors_df.sample(n=sample_size, random_state=42)
    
    # Target
    threshold = donors_df['Lifetime_Giving'].quantile(0.8)
    labels = (donors_df['Lifetime_Giving'] >= threshold).astype(int).values
    print(f"   Target: {np.sum(labels):,} positive ({np.mean(labels)*100:.1f}%)")
    
    # Split
    donor_ids = donors_df['ID'].tolist()
    train_ids, test_ids, y_train, y_test = train_test_split(
        donor_ids, labels, test_size=0.2, random_state=42, stratify=labels
    )
    train_ids, val_ids, y_train, y_val = train_test_split(
        train_ids, y_train, test_size=0.2, random_state=42, stratify=y_train
    )
    
    print(f"‚úÇÔ∏è Split: Train={len(train_ids):,}, Val={len(val_ids):,}, Test={len(test_ids):,}")
    
    # Create datasets
    print("\nüîß Creating datasets...")
    train_ds = WorkingDataset(train_ids, donors_df, giving_df, y_train, fit_scaler=True)
    val_ds = WorkingDataset(val_ids, donors_df, giving_df, y_val, scaler=train_ds.scaler)
    test_ds = WorkingDataset(test_ids, donors_df, giving_df, y_test, scaler=train_ds.scaler)
    
    print(f"   ‚úÖ Features: {train_ds.features.shape[1]} dimensions")
    
    # Model
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nüèãÔ∏è Training on: {device}")
    
    model = WorkingMultimodalModel(
        input_dim=train_ds.features.shape[1],
        hidden_dim=64,
        num_classes=2,
        dropout=0.3
    ).to(device)
    
    print(f"   Parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # DataLoaders
    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
    
    # Optimizer
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)
    
    # Loss
    train_labels = [lbl.item() for lbl in train_ds.labels]
    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)
    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    
    # Training
    print(f"\nüöÄ Starting training...")
    best_val_loss = float('inf')
    patience_counter = 0
    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_f1': []}
    
    for epoch in range(num_epochs):
        # Train
        model.train()
        train_loss = 0
        
        for batch in train_loader:
            optimizer.zero_grad()
            
            features = batch['features'].to(device)
            sequences = batch['sequences'].to(device)
            labels = batch['labels'].to(device)
            
            logits, _ = model(features, sequences)
            loss = criterion(logits, labels)
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            train_loss += loss.item()
        
        train_loss /= len(train_loader)
        
        # Validate
        model.eval()
        val_loss = 0
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for batch in val_loader:
                features = batch['features'].to(device)
                sequences = batch['sequences'].to(device)
                labels = batch['labels'].to(device)
                
                logits, _ = model(features, sequences)
                loss = criterion(logits, labels)
                val_loss += loss.item()
                
                preds = torch.argmax(logits, dim=1)
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        val_loss /= len(val_loader)
        val_acc = accuracy_score(all_labels, all_preds)
        val_f1 = f1_score(all_labels, all_preds, average='weighted')
        
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        history['val_f1'].append(val_f1)
        
        scheduler.step(val_loss)
        
        if epoch % 5 == 0:
            print(f"Epoch {epoch:3d}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}")
        
        # Early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            Path('models').mkdir(exist_ok=True)
            torch.save(model.state_dict(), 'models/best_working_model.pt')
        else:
            patience_counter += 1
            if patience_counter >= 10:
                print(f"üõë Early stopping at epoch {epoch}")
                break
    
    # Load best and test
    if Path('models/best_working_model.pt').exists():
        model.load_state_dict(torch.load('models/best_working_model.pt'))
    
    model.eval()
    test_preds = []
    test_labels = []
    test_probs = []
    
    with torch.no_grad():
        for batch in test_loader:
            features = batch['features'].to(device)
            sequences = batch['sequences'].to(device)
            labels = batch['labels'].to(device)
            
            logits, _ = model(features, sequences)
            probs = torch.softmax(logits, dim=1)
            preds = torch.argmax(logits, dim=1)
            
            test_preds.extend(preds.cpu().numpy())
            test_labels.extend(labels.cpu().numpy())
            test_probs.extend(probs.cpu().numpy())
    
    test_acc = accuracy_score(test_labels, test_preds)
    test_f1 = f1_score(test_labels, test_preds, average='weighted')
    
    if len(np.unique(test_labels)) == 2:
        test_auc = roc_auc_score(test_labels, [p[1] for p in test_probs])
    else:
        test_auc = 0.0
    
    print(f"\nüìä FINAL TEST RESULTS:")
    print(f"   Accuracy: {test_acc:.4f}")
    print(f"   F1 Score: {test_f1:.4f}")
    print(f"   AUC: {test_auc:.4f}")
    
    print(f"\n‚úÖ Training completed successfully - NO NaN!")
    
    return {
        'accuracy': test_acc,
        'f1': test_f1,
        'auc': test_auc,
        'model': model,
        'history': history
    }


if __name__ == "__main__":
    results = train_working_multimodal(sample_size=10000, num_epochs=50, batch_size=64)
    print(f"\nüéâ SUCCESS!")
    print(f"   Accuracy: {results['accuracy']:.4f}")
    print(f"   F1: {results['f1']:.4f}")
    print(f"   AUC: {results['auc']:.4f}")






