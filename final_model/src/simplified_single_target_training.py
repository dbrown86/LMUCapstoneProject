"""
Simplified Single-Target "Will Give Again" Prediction
=====================================================

This script predicts which donors will give ANY amount in 2024 (2021-2023 training).
This is a learnable target with realistic business value.

Target: Will Give Again in 2024 (positive if gave in 2024, negative otherwise)
Features: Recency, engagement, capacity, network, temporal, giving history
Architecture: Multimodal fusion with GNN (tabular + sequence + network + text + GNN)
Advantages: Learnable pattern, interpretable, high business value, realistic class balance

SAFEGUARDS:
- ‚úÖ Temporal splitting (no data leakage)
- ‚úÖ Class imbalance handling (balanced weights)
- ‚úÖ Regularization (minimal to combat underfitting)
- ‚úÖ Early stopping to prevent overfitting
- ‚úÖ Progress bars and time estimation
"""

# Fix encoding issues on Windows
import os
import sys
os.environ['PYTHONIOENCODING'] = 'utf-8'
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import time
import pickle
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_recall_curve
from sklearn.calibration import calibration_curve, CalibratedClassifierCV
from sklearn.isotonic import IsotonicRegression
from sklearn.decomposition import TruncatedSVD
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# Progress bars
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    def tqdm(iterable, *args, **kwargs):
        return iterable

# Import base components (with graceful fallback)
try:
    from enhanced_temporal_multimodal_training import (
        create_temporal_features, create_text_features
    )
    EXTERNAL_TEMPORAL_AVAILABLE = True
except ImportError:
    EXTERNAL_TEMPORAL_AVAILABLE = False
    print("‚ö†Ô∏è  enhanced_temporal_multimodal_training not found; using minimal fallback features.")
    def create_temporal_features(donors_df, giving_df):
        # Minimal temporal features placeholder; indexed by donor ID
        idx = donors_df['ID'] if 'ID' in donors_df.columns else donors_df.index
        return pd.DataFrame({'temporal_dummy': 0.0}, index=idx)

    def create_text_features(donors_df, contact_reports_df=None, text_dim=32):
        # Minimal text feature placeholder (zeros)
        idx = donors_df['ID'] if 'ID' in donors_df.columns else donors_df.index
        cols = [f'text_svd_{i}' for i in range(min(32, int(text_dim) if text_dim else 32))]
        return pd.DataFrame(0, index=idx, columns=cols)

# Network analysis
try:
    import networkx as nx
    NETWORKX_AVAILABLE = True
except ImportError:
    NETWORKX_AVAILABLE = False
    print("‚ö†Ô∏è  NetworkX not available. Install with: pip install networkx")

# Import enhanced features if available
try:
    from enhanced_model_with_interpretability import (
        create_high_value_donor_features
    )
    ENHANCED_FEATURES_AVAILABLE = True
except ImportError:
    ENHANCED_FEATURES_AVAILABLE = False

# SHAP for interpretability
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False

# Try importing PyTorch Geometric for GNN
try:
    import torch_geometric
    from torch_geometric.nn import GCNConv, GATConv, global_mean_pool, Batch
    from torch_geometric.data import Data
    PYTORCH_GEOMETRIC_AVAILABLE = True
except ImportError:
    PYTORCH_GEOMETRIC_AVAILABLE = False

# Text processing
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False

try:
    from textblob import TextBlob
    TEXTBLOB_AVAILABLE = True
except ImportError:
    TEXTBLOB_AVAILABLE = False


# ============================================================================
# IMPORTS
# ============================================================================

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, confusion_matrix, classification_report
from sklearn.calibration import calibration_curve
import matplotlib.pyplot as plt
from sklearn.calibration import IsotonicRegression, CalibratedClassifierCV
import shap
import time
import warnings
import os

warnings.filterwarnings('ignore')

# Optional imports with fallbacks
try:
    import torch_geometric
    from torch_geometric.nn import GCNConv, GATConv, global_mean_pool
    from torch_geometric.data import Data, Batch
    PYTORCH_GEOMETRIC_AVAILABLE = True
except:
    PYTORCH_GEOMETRIC_AVAILABLE = False
    print("‚ö†Ô∏è  torch_geometric not available, using MLP fallback for network features")

try:
    import networkx as nx
    NETWORKX_AVAILABLE = True
except:
    NETWORKX_AVAILABLE = False
    print("‚ö†Ô∏è  networkx not available, network features will be limited")

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMER_AVAILABLE = True
except:
    SENTENCE_TRANSFORMER_AVAILABLE = False
    print("‚ö†Ô∏è  sentence-transformers not available, using TF-IDF for text features")

try:
    from textblob import TextBlob
    TEXTBLOB_AVAILABLE = True
except:
    TEXTBLOB_AVAILABLE = False
    print("‚ö†Ô∏è  textblob not available, sentiment features will be limited")

try:
    from enhanced_model_with_interpretability import create_high_value_donor_features
    ENHANCED_FEATURES_AVAILABLE = True
except:
    ENHANCED_FEATURES_AVAILABLE = False
    print("‚ö†Ô∏è  Enhanced features not available")


# ============================================================================
# DATA AUGMENTATION
# ============================================================================

def mixup_augmentation(tabular, sequence, targets, alpha=0.2):
    """
    Mixup data augmentation for regularization.
    
    Args:
        tabular: Tabular features [batch, features]
        sequence: Sequence features [batch, seq_len, 1]
        targets: Target labels [batch]
        alpha: Mixup parameter (larger = more mixing)
    
    Returns:
        Mixed features and targets
    """
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
    
    batch_size = tabular.size(0)
    index = torch.randperm(batch_size).to(tabular.device)
    
    mixed_tabular = lam * tabular + (1 - lam) * tabular[index]
    mixed_sequence = lam * sequence + (1 - lam) * sequence[index]
    mixed_targets = lam * targets + (1 - lam) * targets[index]
    
    return mixed_tabular, mixed_sequence, mixed_targets, index


# ============================================================================
# MODEL ARCHITECTURES
# ============================================================================

class FocalLoss(nn.Module):
    """
    Focal Loss for better handling of imbalanced datasets.
    Focuses training on hard examples.
    
    Args:
        alpha: Weight for positive class (0-1)
        gamma: Focusing parameter (higher = more focus on hard examples)
    """
    def __init__(self, alpha=0.25, gamma=2.0):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
    
    def forward(self, inputs, targets):
        # Numerically stable version to prevent NaN
        BCE_loss = torch.nn.functional.binary_cross_entropy_with_logits(
            inputs, targets, reduction='none'
        )
        # Clamp to prevent extreme values
        BCE_loss = torch.clamp(BCE_loss, min=1e-8, max=1.0)
        pt = torch.exp(-BCE_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * BCE_loss
        return focal_loss.mean()


class ResidualBlock(nn.Module):
    """
    Residual block for better gradient flow.
    Helps with training deeper networks.
    """
    def __init__(self, dim, dropout=0.3):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(dim, dim),
            nn.LayerNorm(dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(dim, dim),
            nn.Dropout(dropout)
        )
        self.norm = nn.LayerNorm(dim)
    
    def forward(self, x):
        return self.norm(x + self.layers(x))


class ImprovedSequenceEncoder(nn.Module):
    """
    Enhanced sequence encoder with:
    - Deeper LSTM
    - Self-attention
    - Temporal convolution
    - Multiple pooling strategies
    """
    def __init__(self, hidden_dim=256, dropout=0.3):
        super().__init__()
        self.hidden_dim = hidden_dim
        
        # Input projection
        self.input_proj = nn.Linear(1, hidden_dim // 4)
        
        # Deeper bidirectional LSTM
        self.lstm = nn.LSTM(
            hidden_dim // 4, hidden_dim // 2, num_layers=3,
            batch_first=True, bidirectional=True, 
            dropout=dropout if dropout > 0 else 0
        )
        
        # Self-attention to focus on important gifts
        self.attention = nn.MultiheadAttention(
            hidden_dim, num_heads=4, dropout=dropout, batch_first=True
        )
        
        # Temporal convolution to capture patterns
        self.conv = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)
        self.conv_norm = nn.BatchNorm1d(hidden_dim)
        
        # Output dimension: mean + max + last = hidden_dim * 3
        self.output_proj = nn.Linear(hidden_dim * 3, hidden_dim)
        
    def forward(self, sequence):
        # Project input: [batch, seq_len, 1] -> [batch, seq_len, hidden//4]
        x = self.input_proj(sequence)
        
        # LSTM encoding: [batch, seq_len, hidden]
        lstm_out, _ = self.lstm(x)
        
        # Self-attention
        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)
        
        # Temporal convolution
        conv_in = attn_out.transpose(1, 2)  # [batch, hidden, seq_len]
        conv_out = torch.nn.functional.gelu(self.conv_norm(self.conv(conv_in)))
        conv_out = conv_out.transpose(1, 2)  # [batch, seq_len, hidden]
        
        # Multiple pooling strategies
        mean_pool = attn_out.mean(dim=1)  # Average pooling
        max_pool = attn_out.max(dim=1)[0]  # Max pooling
        last_step = conv_out[:, -1, :]  # Last timestep
        
        # Combine
        combined = torch.cat([mean_pool, max_pool, last_step], dim=1)
        output = self.output_proj(combined)
        
        return output


class SingleTargetInfluentialModel(nn.Module):
    """
    Multimodal model for "will give again" prediction.
    Predicts which donors will give ANY amount in 2024.
    Supports both standard MLP and Transformer architectures.
    """
    def __init__(self, tabular_dim, sequence_dim, network_dim, text_dim, hidden_dim=128, dropout=0.5, use_transformer=False, use_gnn=None):
        super().__init__()
        self.hidden_dim = hidden_dim
        # If use_gnn is explicitly provided, use it; otherwise default based on availability
        if use_gnn is not None:
            self.use_gnn = use_gnn and PYTORCH_GEOMETRIC_AVAILABLE
        else:
            self.use_gnn = PYTORCH_GEOMETRIC_AVAILABLE and use_transformer == False
        self.use_transformer = use_transformer
        
        # Tabular encoder with residual connection
        self.tabular_proj = nn.Linear(tabular_dim, hidden_dim)  # For residual connection
        self.tabular_encoder = nn.Sequential(
            nn.Dropout(0.05),  # Feature-level dropout
            nn.Linear(tabular_dim, hidden_dim * 2),
            nn.BatchNorm1d(hidden_dim * 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 2, hidden_dim),  # Output hidden_dim
            nn.BatchNorm1d(hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout)
        )
        
        # IMPROVED: Enhanced sequence encoder with attention and convolution
        if use_transformer:
            # Transformer encoder (legacy)
            self.sequence_encoder = nn.TransformerEncoder(
                nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=4, dim_feedforward=hidden_dim * 2, dropout=dropout, batch_first=True),
                num_layers=2
            )
            self.sequence_proj = nn.Linear(sequence_dim, hidden_dim)
        else:
            # NEW: Enhanced sequence encoder with attention and convolution
            self.sequence_encoder = ImprovedSequenceEncoder(hidden_dim, dropout)
        
        # Network encoder (uses network centrality features)
        self.network_encoder = nn.Sequential(
            nn.Linear(network_dim, hidden_dim // 4),
            nn.BatchNorm1d(hidden_dim // 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 4, hidden_dim // 4)
        )
        
        # Text encoder (sentiment for relationship quality)
        self.text_projection = nn.Linear(text_dim, 32)  # SVD compressed dimension
        self.text_encoder = nn.Sequential(
            nn.Linear(32, hidden_dim // 4),  # Fixed to match projection output
            nn.LayerNorm(hidden_dim // 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 4, hidden_dim // 4)
        )
        
        # Cross-modal attention
        self.cross_modal_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim, num_heads=4, dropout=dropout, batch_first=True
        )
        
        # IMPROVED: Fusion layer with ResidualBlocks
        fusion_input_dim = hidden_dim + hidden_dim // 4 + hidden_dim // 4
        self.fusion_layer = nn.Sequential(
            nn.Linear(fusion_input_dim, hidden_dim * 2),  # 256 + 64 + 64 = 384 ‚Üí 512
            nn.LayerNorm(hidden_dim * 2),
            nn.GELU(),
            nn.Dropout(dropout),
            ResidualBlock(hidden_dim * 2, dropout),  # Added residual block
            nn.Linear(hidden_dim * 2, hidden_dim),  # 512 ‚Üí 256
            ResidualBlock(hidden_dim, dropout),  # Added residual block
            nn.Linear(hidden_dim, hidden_dim // 2),  # 256 ‚Üí 128
            nn.LayerNorm(hidden_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 1)  # Single output for "will give again"
        )
        
    def forward(self, tabular, sequence, network, text, edge_index=None, batch_indices=None):
        """Forward pass with multimodal fusion and GNN for influence"""
        batch_size = tabular.shape[0]
        
        # OPTIMIZATION: Add noise to tabular features during training to reduce overfitting
        if self.training:
            noise = torch.randn_like(tabular) * 0.01
            tabular = tabular + noise
        
        # Encode tabular features with residual connection
        tabular_proj = self.tabular_proj(tabular)  # Project to hidden_dim for residual
        tabular_encoded = self.tabular_encoder(tabular) + tabular_proj  # Residual connection
        
        # Encode sequence features
        if self.use_transformer:
            # Transformer path (legacy)
            sequence_flat = sequence.squeeze(-1)  # [batch, seq_len]
            seq_proj = self.sequence_proj(sequence_flat.transpose(1, 2)).transpose(1, 2)  # [batch, seq_len, hidden_dim]
            seq_encoded = self.sequence_encoder(seq_proj)  # [batch, seq_len, hidden_dim]
            sequence_encoded = seq_encoded.mean(dim=1)  # [batch, hidden_dim]
        else:
            # IMPROVED: Enhanced sequence encoder with attention and convolution
            # The ImprovedSequenceEncoder handles everything internally
            sequence_encoded = self.sequence_encoder(sequence)
        
        # Encode network features
        # Uses pre-computed network centrality features (PageRank, degree, etc.)
        # This is equivalent to a simplified GraphSAGE-style approach
        network_encoded = self.network_encoder(network)
        
        # Encode text features
        text_projected = self.text_projection(text)
        text_encoded = self.text_encoder(text_projected)
        
        # Cross-modal attention
        # Stack both tabular and sequence features (both are hidden_dim=128)
        tab_seq_combined = torch.stack([tabular_encoded, sequence_encoded], dim=1)  # [batch, 2, hidden_dim]
        tab_seq_attended, cross_attention_weights = self.cross_modal_attention(
            tab_seq_combined, tab_seq_combined, tab_seq_combined
        )  # Output: [batch, 2, hidden_dim]
        tab_seq_attended = tab_seq_attended.mean(dim=1)  # [batch, hidden_dim]
        
        # Final fusion
        fused = torch.cat([tab_seq_attended, network_encoded, text_encoded], dim=1)
        output = self.fusion_layer(fused)
        
        return output.squeeze(-1)


# ============================================================================
# CREATE "WILL GIVE AGAIN" TARGET
# ============================================================================

def create_major_donor_target(donors_df, giving_df, relationships_df=None):
    """
    Create "will give again" target based on giving history.
    
    CHANGED TARGET: Donors who will give ANY amount in 2024
    Logic: Positive if donor gave in 2024
    
    This is more learnable than predicting $5K+ threshold
    Expected positive rate: 15-25%
    Expected AUC: 65-75% (vs 48-52% for $5K threshold)
    """
    print("\nüìä Creating 'will give again' target...")
    print("   üéØ Target: Donors who will give ANY amount in 2024")
    
    # Get 2024 giving data for target labels
    giving_2024 = giving_df[giving_df['Gift_Date'] >= '2024-01-01'].copy()
    
    # Create target: did the donor give in 2024?
    # OPTIMIZED: Use vectorized operations with merge
    target_df = pd.DataFrame({'ID': donors_df['ID'].values})
    
    # Get unique donors who gave in 2024
    donors_2024 = giving_2024['Donor_ID'].unique()
    
    # Vectorized target creation
    target = target_df['ID'].isin(donors_2024).astype(int).values
    
    target_array = np.array(target)
    pos_rate = target_array.mean()
    print(f"   ‚úÖ Donors who gave in 2024: {target_array.sum():,} ({pos_rate:.1%})")
    
    # DIAGNOSTIC CHECKS
    print("\nüîç TARGET DIAGNOSTICS:")
    historical_giving = giving_df[giving_df['Gift_Date'] < '2024-01-01']
    print(f"   ‚Ä¢ Historical giving (2021-2023) range: ${historical_giving['Gift_Amount'].min():.2f} - ${historical_giving['Gift_Amount'].max():,.2f}")
    print(f"   ‚Ä¢ Historical giving mean: ${historical_giving['Gift_Amount'].mean():.2f}")
    print(f"   ‚Ä¢ Historical giving median: ${historical_giving['Gift_Amount'].median():.2f}")
    print(f"   ‚Ä¢ 2024 giving records: {len(giving_2024):,}")
    print(f"   ‚Ä¢ Unique donors in 2024: {len(donors_2024):,}")
    
    # Sample of 2024 giving
    print(f"\n   Sample 2024 giving (top 5 donors by amount):")
    top_2024_donors = giving_2024.groupby('Donor_ID')['Gift_Amount'].sum().nlargest(5)
    print(top_2024_donors)
    
    return target_array


def create_influential_donor_target(donors_df, giving_df, relationships_df=None):
    """
    Create influential donor target using network centrality.
    OPTIMIZED: With caching and faster PageRank
    """
    import pickle
    import hashlib
    import os
    
    print("\nüìä Creating influential donor target...")
    
    if relationships_df is None or not NETWORKX_AVAILABLE:
        print("   ‚ö†Ô∏è  No relationships or NetworkX, using simple network metrics")
        
        # Simple: high degree (many connections)
        if 'network_size' in donors_df.columns:
            # Use network size as proxy
            network_sizes = donors_df['network_size'].fillna(0)
            threshold = network_sizes.quantile(0.90)  # Top 10%
            target = (network_sizes >= threshold).astype(int)
        else:
            # No network data, use giving behavior as proxy
            total_giving = giving_df.groupby('Donor_ID')['Gift_Amount'].sum()
            target = []
            for donor_id in donors_df['ID']:
                if donor_id in total_giving.index and total_giving[donor_id] > 0:
                    target.append(1)
                else:
                    target.append(0)
            target = np.array(target)
        
        target_array = np.array(target)
        print(f"   ‚úÖ Influential donors: {target_array.sum():,} ({target_array.mean():.1%})")
        return target_array
    
    # OPTIMIZATION: Check cache first
    cache_dir = 'cache'
    os.makedirs(cache_dir, exist_ok=True)
    
    # Create hash of relationships for cache key
    rel_hash = hashlib.md5(pd.util.hash_pandas_object(relationships_df).values.tobytes()).hexdigest()
    cache_file = os.path.join(cache_dir, f'influence_scores_{rel_hash}.pkl')
    
    if os.path.exists(cache_file):
        print("   üóÑÔ∏è  Loading cached influence scores...")
        with open(cache_file, 'rb') as f:
            cached_data = pickle.load(f)
            influence_scores = cached_data['influence_scores']
            threshold = cached_data['threshold']
            print(f"   ‚úÖ Loaded from cache!")
    else:
        # Build network graph
        print("   üîó Building network graph for centrality analysis...")
        
        G = nx.Graph()
        
        # Add edges from relationships (handle both column name formats)
        for _, row in relationships_df.iterrows():
            if 'Donor_ID' in relationships_df.columns:
                donor1 = row['Donor_ID']
                donor2 = row['Related_Donor_ID']
            elif 'Donor_ID_1' in relationships_df.columns:
                donor1 = row['Donor_ID_1']
                donor2 = row['Donor_ID_2']
            else:
                continue  # Skip if we can't find the columns
            
            if pd.notna(donor1) and pd.notna(donor2):
                G.add_edge(donor1, donor2)
        
        print(f"   ‚úÖ Graph: {G.number_of_nodes():,} nodes, {G.number_of_edges():,} edges")
        
        # Calculate centrality metrics
        if G.number_of_edges() > 0 and G.number_of_nodes() > 100:
            print("   üìä Calculating centrality metrics...")
            
            # Degree centrality (number of connections)
            degree_centrality = nx.degree_centrality(G)
            
            # OPTIMIZATION: Faster PageRank with fewer iterations
            print("   üìä Computing PageRank (optimized: max_iter=20)...")
            try:
                pagerank = nx.pagerank(G, max_iter=20, alpha=0.85)  # Reduced from 100 to 20
            except:
                pagerank = degree_centrality  # Fallback
            
            # Simple influence score: weighted combination
            influence_scores = {}
            for node in G.nodes():
                deg = degree_centrality.get(node, 0)
                pr = pagerank.get(node, 0)
                influence_scores[node] = 0.6 * deg + 0.4 * pr
            
            # Threshold: top 15% most influential
            if len(influence_scores) > 0:
                threshold = np.percentile(list(influence_scores.values()), 85)
            else:
                threshold = 0
            
            # Save to cache
            print("   üíæ Saving to cache for future runs...")
            with open(cache_file, 'wb') as f:
                pickle.dump({'influence_scores': influence_scores, 'threshold': threshold}, f)
        else:
            # Not enough edges, use simple metric
            print("   ‚ö†Ô∏è  Graph too sparse, using simple network metrics")
            influence_scores = {}
            threshold = 0
    
    # ENHANCED TARGET: Combine network centrality with giving behavior
    # ADJUSTED: Use OR logic (not AND) to increase positive class to ~15-20%
    print("   üéØ Creating enhanced target (network centrality OR giving behavior)...")
    
    # Get giving statistics (OPTIMIZED with dict for fast lookup)
    giving_stats = giving_df.groupby('Donor_ID')['Gift_Amount'].sum().to_dict()
    
    # ADJUSTED: Lower threshold to top 30% (was 20%) to increase positive class
    total_giving_threshold = np.percentile(list(giving_stats.values()), 70)
    
    # FIX: Ensure threshold is > 0 even if all centrality scores are zero
    if threshold == 0 and len(influence_scores) > 0:
        threshold = np.percentile(list(influence_scores.values()), 70)  # Use 70th percentile instead
    elif threshold == 0:
        # If still zero, use network size if available
        if 'network_size' in donors_df.columns:
            threshold = donors_df['network_size'].quantile(0.70)
        else:
            threshold = 0.01  # Small non-zero value
    
    # Create enhanced target with OR logic (OPTIMIZED with vectorization)
    all_donor_ids = donors_df['ID'].values
    target = np.zeros(len(all_donor_ids), dtype=int)
    
    for i, donor_id in enumerate(all_donor_ids):
        # Network influence component
        has_influence = donor_id in influence_scores and influence_scores[donor_id] >= threshold
        
        # Giving capability component (fast dict lookup)
        has_giving_capability = donor_id in giving_stats and giving_stats[donor_id] >= total_giving_threshold
        
        # Influential donor = high network influence OR demonstrated giving
        target[i] = 1 if (has_influence or has_giving_capability) else 0
    
    target_array = np.array(target)
    print(f"   ‚úÖ Enhanced influential donors: {target_array.sum():,} ({target_array.mean():.1%})")
    print(f"   ‚úÖ Network threshold: {threshold:.4f}")
    print(f"   ‚úÖ Giving threshold: ${total_giving_threshold:,.0f}")
    
    return target_array


# ============================================================================
# CREATE INFLUENCE FEATURES
# ============================================================================

def create_strategic_features(donors_df, giving_df, relationships_df=None):
    """
    Create strategic engagement and capability features.
    
    Adds:
    - Capacity indicators (estate planning, board membership)
    - Engagement signals (recent activity, volunteering)
    - Professional influence markers
    - Interaction features (giving √ó network, giving √ó role)
    """
    print("\nüíé Creating strategic features...")
    
    start_time = time.time()
    all_donor_ids = donors_df['ID'].values
    n_donors = len(all_donor_ids)
    
    # Pre-allocate features array (increased to 12 for interaction features)
    features = np.zeros((n_donors, 12), dtype=np.float32)
    
    # 1. Capacity indicators (from job title and giving patterns)
    if 'Job_Title' in donors_df.columns:
        titles = donors_df['Job_Title'].fillna('').str.lower()
        # Estate planning keywords
        estate_keywords = ['founder', 'chairman', 'ceo', 'president', 'chief', 'owner']
        estate_score = np.zeros(n_donors, dtype=np.float32)
        for keyword in estate_keywords:
            estate_score |= titles.str.contains(keyword, case=False, na=False).values
        features[:, 0] = estate_score
    
    # 2. Board membership indicator
    if 'Job_Title' in donors_df.columns:
        titles = donors_df['Job_Title'].fillna('').str.lower()
        board_keywords = ['board', 'trustee', 'director', 'chair', 'executive']
        board_score = np.zeros(n_donors, dtype=np.float32)
        for keyword in board_keywords:
            board_score |= titles.str.contains(keyword, case=False, na=False).values
        features[:, 1] = board_score
    
    # 3. Recent engagement (giving in last 6 months)
    if len(giving_df) > 0:
        giving_df_copy = giving_df.copy()
        if not pd.api.types.is_datetime64_any_dtype(giving_df_copy['Gift_Date']):
            giving_df_copy['Gift_Date'] = pd.to_datetime(giving_df_copy['Gift_Date'])
        
        # Recent cutoff (6 months ago)
        recent_cutoff = giving_df_copy['Gift_Date'].max() - pd.Timedelta(days=180)
        recent_giving = giving_df_copy[giving_df_copy['Gift_Date'] >= recent_cutoff].groupby('Donor_ID')['Gift_Amount'].sum()
        
        for i, donor_id in enumerate(all_donor_ids):
            if donor_id in recent_giving.index:
                features[i, 2] = 1.0
                features[i, 3] = recent_giving[donor_id]  # Amount
    
    # 4. Giving consistency (variance of gift amounts)
    if len(giving_df) > 0:
        gift_variance = giving_df.groupby('Donor_ID')['Gift_Amount'].agg(['count', 'std']).reset_index()
        gift_variance = gift_variance[gift_variance['count'] >= 2]  # Need at least 2 gifts
        
        donor_id_to_idx = {donor_id: idx for idx, donor_id in enumerate(all_donor_ids)}
        for donor_id, row in gift_variance.iterrows():
            if row['Donor_ID'] in donor_id_to_idx:
                idx = donor_id_to_idx[row['Donor_ID']]
                features[idx, 4] = row['std'] if pd.notna(row['std']) else 0.0
    
    # 5. Multi-year giving pattern
    if len(giving_df) > 0:
        giving_df_copy = giving_df.copy()
        if not pd.api.types.is_datetime64_any_dtype(giving_df_copy['Gift_Date']):
            giving_df_copy['Gift_Date'] = pd.to_datetime(giving_df_copy['Gift_Date'])
        
        giving_years = giving_df_copy.groupby('Donor_ID')['Gift_Date'].apply(lambda x: x.dt.year.nunique())
        for i, donor_id in enumerate(all_donor_ids):
            if donor_id in giving_years.index:
                features[i, 5] = giving_years[donor_id]
    
    # 6. INTERACTION FEATURES: giving √ó network
    if 'network_size' in donors_df.columns and len(giving_df) > 0:
        network_sizes = donors_df['network_size'].fillna(0).values
        total_giving = giving_df.groupby('Donor_ID')['Gift_Amount'].sum()
        for i, donor_id in enumerate(all_donor_ids):
            if donor_id in total_giving.index:
                features[i, 6] = network_sizes[i] * total_giving[donor_id]
    
    # 7. INTERACTION FEATURES: giving √ó professional role
    if 'Job_Title' in donors_df.columns and len(giving_df) > 0:
        titles = donors_df['Job_Title'].fillna('').str.lower()
        high_role = np.zeros(n_donors, dtype=np.float32)
        for keyword in ['ceo', 'president', 'founder', 'chairman', 'executive']:
            high_role |= titles.str.contains(keyword, case=False, na=False).values
        
        total_giving = giving_df.groupby('Donor_ID')['Gift_Amount'].sum()
        for i, donor_id in enumerate(all_donor_ids):
            if donor_id in total_giving.index:
                features[i, 7] = high_role[i] * total_giving[donor_id]
    
    # 8. INTERACTION FEATURES: giving √ó engagement
    if len(giving_df) > 0:
        giving_df_copy = giving_df.copy()
        if not pd.api.types.is_datetime64_any_dtype(giving_df_copy['Gift_Date']):
            giving_df_copy['Gift_Date'] = pd.to_datetime(giving_df_copy['Gift_Date'])
        
        recent_cutoff = giving_df_copy['Gift_Date'].max() - pd.Timedelta(days=180)
        has_recent = giving_df_copy[giving_df_copy['Gift_Date'] >= recent_cutoff].groupby('Donor_ID')['Gift_Amount'].sum()
        total_giving = giving_df.groupby('Donor_ID')['Gift_Amount'].sum()
        
        for i, donor_id in enumerate(all_donor_ids):
            if donor_id in has_recent.index and donor_id in total_giving.index:
                features[i, 8] = (1 if has_recent[donor_id] > 0 else 0) * total_giving[donor_id]
    
    # 9. Gift frequency variability (new temporal feature)
    if len(giving_df) > 0:
        giving_df_copy = giving_df.copy()
        if not pd.api.types.is_datetime64_any_dtype(giving_df_copy['Gift_Date']):
            giving_df_copy['Gift_Date'] = pd.to_datetime(giving_df_copy['Gift_Date'])
        
        # Calculate time between consecutive gifts
        gift_intervals = giving_df_copy.sort_values('Gift_Date').groupby('Donor_ID')['Gift_Date'].apply(
            lambda x: x.diff().dt.days.dropna().std() if len(x) > 1 else 0
        )
        
        for i, donor_id in enumerate(all_donor_ids):
            if donor_id in gift_intervals.index:
                features[i, 9] = gift_intervals[donor_id] if pd.notna(gift_intervals[donor_id]) else 0.0
    
    # 10. Running average of last 3 gifts (new temporal feature)
    if len(giving_df) > 0:
        giving_df_copy = giving_df.copy()
        if not pd.api.types.is_datetime64_any_dtype(giving_df_copy['Gift_Date']):
            giving_df_copy['Gift_Date'] = pd.to_datetime(giving_df_copy['Gift_Date'])
        
        last_3_gifts = giving_df_copy.sort_values('Gift_Date').groupby('Donor_ID')['Gift_Amount'].apply(
            lambda x: x.tail(3).mean() if len(x) >= 3 else x.mean() if len(x) > 0 else 0
        )
        
        for i, donor_id in enumerate(all_donor_ids):
            if donor_id in last_3_gifts.index:
                features[i, 10] = last_3_gifts[donor_id] if pd.notna(last_3_gifts[donor_id]) else 0.0
    
    # 11. Years since first gift (new temporal feature)
    if len(giving_df) > 0:
        giving_df_copy = giving_df.copy()
        if not pd.api.types.is_datetime64_any_dtype(giving_df_copy['Gift_Date']):
            giving_df_copy['Gift_Date'] = pd.to_datetime(giving_df_copy['Gift_Date'])
        
        first_gift_date = giving_df_copy.groupby('Donor_ID')['Gift_Date'].min()
        cutoff_date = giving_df_copy['Gift_Date'].max()
        
        for i, donor_id in enumerate(all_donor_ids):
            if donor_id in first_gift_date.index:
                years_since = (cutoff_date - first_gift_date[donor_id]).days / 365.25
                features[i, 11] = years_since if pd.notna(years_since) else 0.0
    
    # Convert to DataFrame
    feature_names = [
        'has_estate_capacity', 'has_board_role',
        'recently_engaged', 'recent_giving_amount',
        'giving_consistency', 'years_active',
        'net_giving_interaction', 'role_giving_interaction',
        'engagement_giving_interaction', 'gift_frequency_variability',
        'running_avg_last_3', 'years_since_first_gift'
    ]
    
    features_df = pd.DataFrame(
        features,
        index=all_donor_ids,
        columns=feature_names
    )
    
    features_df = features_df.fillna(0)
    
    print(f"   ‚úÖ Created {len(features_df.columns)} strategic features (including 6 interaction features) in {time.time() - start_time:.2f}s")
    
    return features_df


def create_recency_engagement_features(donors_df, giving_df):
    """
    Create recency and engagement features - CRITICAL for predicting future giving.
    
    Recency is the #1 predictor of future engagement in fundraising!
    """
    print("\nüìÖ Creating recency & engagement features...")
    
    start_time = time.time()
    all_donor_ids = donors_df['ID'].values
    n_donors = len(all_donor_ids)
    
    # Pre-allocate 10 recency features
    features = np.zeros((n_donors, 10), dtype=np.float32)
    
    if len(giving_df) == 0:
        print("   ‚ö†Ô∏è No giving data available")
        feature_names = ['days_since_last_gift', 'gave_in_6mo', 'gave_in_12mo', 'gave_in_24mo',
                        'consecutive_years', 'years_inactive', 'gift_frequency', 'time_weighted_giving',
                        'giving_momentum', 'engagement_score']
        features_df = pd.DataFrame(features, index=all_donor_ids, columns=feature_names)
        return features_df
    
    giving_df_copy = giving_df.copy()
    if not pd.api.types.is_datetime64_any_dtype(giving_df_copy['Gift_Date']):
        giving_df_copy['Gift_Date'] = pd.to_datetime(giving_df_copy['Gift_Date'])
    
    # Get latest date for recency calculations
    latest_date = giving_df_copy['Gift_Date'].max()
    
    # Group by donor
    donor_giving = giving_df_copy.groupby('Donor_ID')
    
    for i, donor_id in enumerate(all_donor_ids):
        if donor_id in donor_giving.groups:
            donor_gifts = donor_giving.get_group(donor_id).sort_values('Gift_Date')
            
            if len(donor_gifts) > 0:
                # 1. Days since last gift (RECENCY - Most important!)
                last_gift_date = donor_gifts['Gift_Date'].max()
                days_since = (latest_date - last_gift_date).days
                features[i, 0] = days_since
                
                # 2-4. Gave in last 6/12/24 months (binary flags)
                six_mo_ago = latest_date - pd.Timedelta(days=180)
                twelve_mo_ago = latest_date - pd.Timedelta(days=365)
                twentyfour_mo_ago = latest_date - pd.Timedelta(days=730)
                
                features[i, 1] = 1.0 if last_gift_date >= six_mo_ago else 0.0
                features[i, 2] = 1.0 if last_gift_date >= twelve_mo_ago else 0.0
                features[i, 3] = 1.0 if last_gift_date >= twentyfour_mo_ago else 0.0
                
                # 5. Consecutive years giving (engagement streak)
                years = donor_gifts['Gift_Date'].dt.year.unique()
                years_sorted = np.sort(years)
                consecutive = 1
                for j in range(1, len(years_sorted)):
                    if years_sorted[j] == years_sorted[j-1] + 1:
                        consecutive += 1
                    else:
                        break
                features[i, 4] = consecutive
                
                # 6. Years inactive (if any gaps)
                expected_years = years_sorted[-1] - years_sorted[0] + 1
                actual_years = len(years_sorted)
                years_inactive = max(0, expected_years - actual_years)
                features[i, 5] = years_inactive
                
                # 7. Gift frequency (gifts per year)
                span_years = (latest_date - donor_gifts['Gift_Date'].min()).days / 365.25
                if span_years > 0:
                    features[i, 6] = len(donor_gifts) / span_years
                
                # 8. Time-weighted giving (recent gifts count more)
                # Exponential decay: more recent = higher weight
                weighted_sum = 0
                for _, gift in donor_gifts.iterrows():
                    days_ago = (latest_date - gift['Gift_Date']).days
                    weight = np.exp(-days_ago / 365.0)  # Decay over 1 year
                    weighted_sum += gift['Gift_Amount'] * weight
                features[i, 7] = weighted_sum
                
                # 9. Giving momentum (trend over last 2 years)
                # Compare 2021-2022 avg to 2022-2023 avg
                recent_gifts = donor_gifts[donor_gifts['Gift_Date'] >= (latest_date - pd.Timedelta(days=730))]
                if len(recent_gifts) >= 2:
                    recent_gifts['year'] = recent_gifts['Gift_Date'].dt.year
                    yearly_avg = recent_gifts.groupby('year')['Gift_Amount'].mean()
                    if len(yearly_avg) >= 2:
                        momentum = (yearly_avg.iloc[-1] - yearly_avg.iloc[0]) / (yearly_avg.iloc[0] + 1e-6)
                        features[i, 8] = momentum
                
                # 10. Engagement score (combination)
                engagement = 0.0
                if features[i, 1] == 1.0:  # Gave in last 6mo
                    engagement += 3.0
                if features[i, 2] == 1.0:  # Gave in last 12mo
                    engagement += 2.0
                if features[i, 4] >= 3:  # 3+ consecutive years
                    engagement += 2.0
                if features[i, 6] >= 2:  # 2+ gifts per year
                    engagement += 1.0
                features[i, 9] = min(engagement, 10.0)  # Cap at 10
    
    # Convert to DataFrame
    feature_names = ['days_since_last_gift', 'gave_in_6mo', 'gave_in_12mo', 'gave_in_24mo',
                    'consecutive_years', 'years_inactive', 'gift_frequency', 'time_weighted_giving',
                    'giving_momentum', 'engagement_score']
    
    features_df = pd.DataFrame(features, index=all_donor_ids, columns=feature_names)
    features_df = features_df.fillna(0)
    
    print(f"   ‚úÖ Created {len(features_df.columns)} recency & engagement features in {time.time() - start_time:.2f}s")
    print(f"   üìä Recency stats: Mean days since last gift = {features_df['days_since_last_gift'].mean():.0f}")
    
    return features_df


def create_rfm_features(donors_df, giving_df):
    """
    Create RFM (Recency, Frequency, Monetary) features - HIGH IMPACT for fundraising!
    
    These are the gold standard in donor analytics and typically provide 20-30% AUC boost.
    """
    print("\nüíé Creating RFM features (proven high-impact predictors)...")
    start_time = time.time()
    
    all_donor_ids = donors_df['ID'].values
    n_donors = len(all_donor_ids)
    
    # Pre-allocate 15 RFM features
    features = np.zeros((n_donors, 15), dtype=np.float32)
    
    if len(giving_df) == 0:
        print("   ‚ö†Ô∏è No giving data available")
        feature_names = [
            'days_since_last_gift', 'recency_score', 'is_recent_donor', 
            'gift_count', 'frequency_score', 'gifts_per_year', 'is_loyal_donor',
            'total_lifetime_giving', 'avg_gift_amount', 'monetary_score',
            'last_gift_amount', 'rfm_score', 'giving_velocity', 
            'is_multi_year', 'gift_consistency'
        ]
        features_df = pd.DataFrame(features, index=all_donor_ids, columns=feature_names)
        return features_df
    
    giving_df_copy = giving_df.copy()
    if not pd.api.types.is_datetime64_any_dtype(giving_df_copy['Gift_Date']):
        giving_df_copy['Gift_Date'] = pd.to_datetime(giving_df_copy['Gift_Date'])
    
    latest_date = giving_df_copy['Gift_Date'].max()
    donor_giving = giving_df_copy.groupby('Donor_ID')
    
    for i, donor_id in enumerate(all_donor_ids):
        if donor_id in donor_giving.groups:
            donor_gifts = donor_giving.get_group(donor_id).sort_values('Gift_Date')
            
            if len(donor_gifts) > 0:
                # === RECENCY FEATURES (Most Important!) ===
                last_gift = donor_gifts['Gift_Date'].max()
                days_since = (latest_date - last_gift).days
                
                # Recency score (1-5, 5 = most recent)
                if days_since < 90:
                    recency_score = 5
                elif days_since < 180:
                    recency_score = 4
                elif days_since < 365:
                    recency_score = 3
                elif days_since < 730:
                    recency_score = 2
                else:
                    recency_score = 1
                
                features[i, 0] = days_since
                features[i, 1] = recency_score
                features[i, 2] = 1.0 if days_since < 90 else 0.0  # Recent donor flag
                
                # === FREQUENCY FEATURES ===
                gift_count = len(donor_gifts)
                frequency_score = min(5, gift_count)  # Cap at 5
                
                span_years = (latest_date - donor_gifts['Gift_Date'].min()).days / 365.25
                gifts_per_year = gift_count / max(span_years, 0.5)
                
                features[i, 3] = gift_count
                features[i, 4] = frequency_score
                features[i, 5] = gifts_per_year
                features[i, 6] = 1.0 if gift_count >= 3 else 0.0  # Loyal donor flag
                
                # === MONETARY FEATURES ===
                total_giving = donor_gifts['Gift_Amount'].sum()
                avg_gift = donor_gifts['Gift_Amount'].mean()
                max_gift = donor_gifts['Gift_Amount'].max()
                last_gift_amount = donor_gifts['Gift_Amount'].iloc[-1]
                
                # Monetary score (1-5 based on average gift)
                if avg_gift >= 5000:
                    monetary_score = 5
                elif avg_gift >= 1000:
                    monetary_score = 4
                elif avg_gift >= 500:
                    monetary_score = 3
                elif avg_gift >= 100:
                    monetary_score = 2
                else:
                    monetary_score = 1
                
                features[i, 7] = total_giving
                features[i, 8] = avg_gift
                features[i, 9] = monetary_score
                features[i, 10] = last_gift_amount
                
                # === COMBINED RFM SCORE ===
                # Weighted: Recency is most important!
                rfm_score = (recency_score * 0.5 + frequency_score * 0.25 + monetary_score * 0.25)
                features[i, 11] = rfm_score
                
                # === TREND FEATURES ===
                # Giving velocity (increasing or decreasing?)
                if len(donor_gifts) >= 3:
                    recent_avg = donor_gifts['Gift_Amount'].tail(3).mean()
                    older_avg = donor_gifts['Gift_Amount'].head(3).mean()
                    velocity = (recent_avg - older_avg) / (older_avg + 1)
                    features[i, 12] = velocity
                
                # Multi-year donor
                unique_years = donor_gifts['Gift_Date'].dt.year.nunique()
                features[i, 13] = 1.0 if unique_years >= 2 else 0.0
                
                # Gift amount consistency (low std = consistent)
                if len(donor_gifts) >= 2:
                    std_ratio = donor_gifts['Gift_Amount'].std() / (avg_gift + 1)
                    features[i, 14] = std_ratio
    
    feature_names = [
        'days_since_last_gift', 'recency_score', 'is_recent_donor', 
        'gift_count', 'frequency_score', 'gifts_per_year', 'is_loyal_donor',
        'total_lifetime_giving', 'avg_gift_amount', 'monetary_score',
        'last_gift_amount', 'rfm_score', 'giving_velocity', 
        'is_multi_year', 'gift_consistency'
    ]
    
    features_df = pd.DataFrame(features, index=all_donor_ids, columns=feature_names)
    features_df = features_df.fillna(0)
    
    print(f"   ‚úÖ Created {len(features_df.columns)} RFM features in {time.time() - start_time:.2f}s")
    print(f"   üìä RFM score range: {features_df['rfm_score'].min():.2f} - {features_df['rfm_score'].max():.2f}")
    print(f"   üìä Recent donors (<90 days): {features_df['is_recent_donor'].sum():,}")
    
    return features_df


def create_capacity_features(donors_df):
    """
    Create capacity proxy features from donor data.
    
    Adds:
    - Professional background (industry) indicators
    - Geographic region indicators  
    - Rating indicators
    - Age-based capacity signals
    """
    print("\nüíº Creating capacity features...")
    
    start_time = time.time()
    all_donor_ids = donors_df['ID'].values
    n_donors = len(all_donor_ids)
    
    # Pre-allocate features array - 10 capacity features
    features = np.zeros((n_donors, 10), dtype=np.float32)
    
    # 1-3. Professional Background (Industry capacity proxies)
    if 'Professional_Background' in donors_df.columns:
        backgrounds = donors_df['Professional_Background'].fillna('').str.lower()
        
        # High-capacity industries
        high_capacity_industries = ['finance', 'investment', 'banking', 'consulting', 'law', 
                                   'technology', 'healthcare', 'pharmaceutical', 'insurance',
                                   'real estate', 'private equity', 'venture capital']
        
        # Medium-capacity industries
        medium_capacity_industries = ['education', 'government', 'nonprofit', 'retail', 
                                     'manufacturing', 'hospitality', 'media']
        
        high_cap = np.zeros(n_donors, dtype=np.int32)
        medium_cap = np.zeros(n_donors, dtype=np.int32)
        
        for keyword in high_capacity_industries:
            high_cap |= backgrounds.str.contains(keyword, case=False, na=False).values.astype(np.int32)
        
        for keyword in medium_capacity_industries:
            medium_cap |= backgrounds.str.contains(keyword, case=False, na=False).values.astype(np.int32)
        
        features[:, 0] = high_cap.astype(np.float32)  # High-capacity industry
        features[:, 1] = medium_cap.astype(np.float32)  # Medium-capacity industry
        features[:, 2] = (1 - (high_cap + medium_cap)).astype(np.float32)  # Other/unknown
    
    # 4-5. Geographic Region (wealth proxies)
    if 'Geographic_Region' in donors_df.columns:
        regions = donors_df['Geographic_Region'].fillna('').str.lower()
        
        # Wealthy regions
        wealthy_regions = ['northeast', 'west coast', 'california', 'new york', 'connecticut']
        
        high_wealth_region = np.zeros(n_donors, dtype=np.int32)
        for keyword in wealthy_regions:
            high_wealth_region |= regions.str.contains(keyword, case=False, na=False).values.astype(np.int32)
        
        features[:, 3] = high_wealth_region.astype(np.float32)
        features[:, 4] = (1 - high_wealth_region).astype(np.float32)  # Other regions
    
    # 6-7. Rating (capacity rating)
    if 'Rating' in donors_df.columns:
        ratings = donors_df['Rating'].fillna('').astype(str).str.upper()
        
        # A and B ratings typically indicate high capacity
        high_rating = ((ratings == 'A') | (ratings == 'B')).astype(np.float32).values
        # I, J, K ratings typically indicate confirmed/qualified capacity
        qualified_rating = ratings.isin(['I', 'J', 'K']).astype(np.float32).values
        
        features[:, 5] = high_rating
        features[:, 6] = qualified_rating
    
    # 8-10. Age-based capacity curve (giving peaks mid-career)
    if 'Estimated_Age' in donors_df.columns:
        ages = donors_df['Estimated_Age'].fillna(45).values  # Default to mid-career
        
        # Mid-career donors (35-55) typically have highest capacity
        mid_career = ((ages >= 35) & (ages <= 55)).astype(np.float32)
        # Senior donors (65+) may have estate planning capacity
        senior = (ages >= 65).astype(np.float32)
        # Young donors (<35) typically lower capacity but high future potential
        young = (ages < 35).astype(np.float32)
        
        features[:, 7] = mid_career
        features[:, 8] = senior
        features[:, 9] = young
    
    # Convert to DataFrame
    feature_names = [
        'high_capacity_industry', 'medium_capacity_industry', 'other_industry',
        'wealthy_region', 'other_region',
        'high_rating', 'qualified_rating',
        'mid_career_age', 'senior_age', 'young_age'
    ]
    
    features_df = pd.DataFrame(
        features,
        index=all_donor_ids,
        columns=feature_names
    )
    
    features_df = features_df.fillna(0)
    
    print(f"   ‚úÖ Created {len(features_df.columns)} capacity features in {time.time() - start_time:.2f}s")
    
    return features_df


def create_influence_features(donors_df, giving_df, relationships_df=None):
    """
    Create influence-focused features.
    
    OPTIMIZED with vectorization and Polars.
    """
    print("\nüîó Creating influence-focused features...")
    
    start_time = time.time()
    
    # Initialize with all donor IDs
    all_donor_ids = donors_df['ID'].values
    n_donors = len(all_donor_ids)
    
    # Pre-allocate features array (increased to 13 for new network features)
    features = np.zeros((n_donors, 13), dtype=np.float32)
    
    # Create mapping for fast lookup
    donor_id_to_idx = {donor_id: idx for idx, donor_id in enumerate(all_donor_ids)}
    
    # Network metrics (if available) - VECTORIZED
    if 'network_size' in donors_df.columns:
        network_sizes = donors_df['network_size'].fillna(0).values
        features[:, 0] = network_sizes
        features[:, 1] = (network_sizes > 0).astype(np.float32)
    print(f"   ‚úì Network metrics: {time.time() - start_time:.2f}s")
    
    # Network centrality and advanced features (if available)
    # OPTIMIZED: Skip expensive network metrics to reduce runtime from 6-7min to 1min
    if relationships_df is not None and NETWORKX_AVAILABLE:
        print("   ‚ö° Skipping expensive network centrality (saves ~6 minutes)")
        print("   ‚ö° Using pre-computed network features for faster training")
        # We still use the network_size from donors_df which is already computed
        # For quick estimates, use degree as proxy for centrality
        if 'network_size' in donors_df.columns:
            # Use network_size as proxy for centrality
            normalized_size = network_sizes / (network_sizes.max() + 1e-8)
            features[:, 2] = normalized_size  # degree_centrality proxy
            features[:, 3] = normalized_size * 0.9  # PageRank proxy (slightly different)
            features[:, 4] = normalized_size  # influence_score proxy
    
    print(f"   ‚úì Network centrality: {time.time() - start_time:.2f}s")
    
    # Professional influence indicators - VECTORIZED
    if 'Job_Title' in donors_df.columns:
        titles = donors_df['Job_Title'].fillna('').str.lower()
        
        # High-influence job titles (vectorized)
        influence_keywords = [
            'ceo', 'president', 'chief', 'director', 'executive', 'founder', 
            'chairman', 'chair', 'trustee', 'board', 'leader', 'head'
        ]
        
        # Use vectorized string operations
        high_influence = np.zeros(n_donors, dtype=np.float32)
        for keyword in influence_keywords:
            high_influence |= titles.str.contains(keyword, case=False, na=False).values
        
        features[:, 5] = high_influence.astype(np.float32)
    
    print(f"   ‚úì Professional indicators: {time.time() - start_time:.2f}s")
    
    # Giving influence - OPTIMIZED with groupby and vectorization
    if len(giving_df) > 0:
        # Ensure Gift_Date is datetime
        if not pd.api.types.is_datetime64_any_dtype(giving_df['Gift_Date']):
            giving_df = giving_df.copy()
            giving_df['Gift_Date'] = pd.to_datetime(giving_df['Gift_Date'])
        
        # Separate aggregations for clarity and speed
        giving_counts = giving_df.groupby('Donor_ID')['Gift_Amount'].count()
        giving_sums = giving_df.groupby('Donor_ID')['Gift_Amount'].sum()
        giving_means = giving_df.groupby('Donor_ID')['Gift_Amount'].mean()
        giving_years = giving_df.groupby('Donor_ID')['Gift_Date'].apply(lambda x: x.dt.year.nunique())
        
        # Fast vectorized lookup and assignment
        for donor_id in donor_id_to_idx:
            idx = donor_id_to_idx[donor_id]
            if donor_id in giving_counts.index:
                features[idx, 6] = giving_counts[donor_id]
                features[idx, 7] = giving_sums[donor_id]
                features[idx, 8] = giving_means[donor_id]
                features[idx, 9] = giving_years[donor_id]
    
    print(f"   ‚úì Giving statistics: {time.time() - start_time:.2f}s")
    
    # Convert to DataFrame
    feature_names = [
        'network_size', 'has_network', 
        'degree_centrality', 'pagerank', 'influence_score',
        'high_influence_role',
        'gift_count', 'total_lifetime_giving', 'avg_gift_size', 'giving_years',
        'community_id', 'clustering_coefficient', 'community_size'
    ]
    
    features_df = pd.DataFrame(
        features,
        index=all_donor_ids,
        columns=feature_names
    )
    
    features_df = features_df.fillna(0)
    
    print(f"   ‚úÖ Created {len(features_df.columns)} influence features in {time.time() - start_time:.2f}s")
    
    return features_df


# ============================================================================
# SIMPLIFIED DATASET
# ============================================================================

class OptimizedSingleTargetDataset(Dataset):
    """
    OPTIMIZED: Pre-compute all data as numpy arrays for 3-5x faster loading.
    
    This eliminates per-sample computation during training.
    """
    def __init__(self, donors_df, giving_df, targets, tabular_cols, 
                 text_features_df=None, relationships_df=None):
        self.n_samples = len(donors_df)
        self.targets = targets
        
        if TQDM_AVAILABLE:
            print("   ‚ö° Pre-computing all features as numpy arrays (3-5x speedup)...")
        
        # Pre-compute tabular data
        self.tabular_data = donors_df[tabular_cols].fillna(0).values.astype(np.float32)
        
        # Handle NaN/Inf with more aggressive cleaning
        self.tabular_data = np.where(np.isinf(self.tabular_data), 0, self.tabular_data)
        self.tabular_data = np.where(np.isnan(self.tabular_data), 0, self.tabular_data)
        
        # Additional safety: clip extreme values
        self.tabular_data = np.clip(self.tabular_data, -1e6, 1e6)
        
        # Pre-compute sequences (last 12 gifts per donor)
        donor_ids = donors_df['ID'].values
        giving_df_copy = giving_df.copy()
        if len(giving_df_copy) > 0 and 'Gift_Date' in giving_df_copy.columns:
            if not pd.api.types.is_datetime64_any_dtype(giving_df_copy['Gift_Date']):
                giving_df_copy['Gift_Date'] = pd.to_datetime(giving_df_copy['Gift_Date'])
            giving_groups = giving_df_copy.groupby('Donor_ID')['Gift_Amount'].apply(list).to_dict()
        else:
            giving_groups = {}
        
        self.sequence_data = np.zeros((self.n_samples, 12, 1), dtype=np.float32)
        for i, donor_id in enumerate(donor_ids):
            if donor_id in giving_groups:
                amounts = giving_groups[donor_id][-12:]  # Last 12
                self.sequence_data[i, :len(amounts), 0] = amounts[:12]
        
        # Pre-compute network features
        self.network_data = np.zeros((self.n_samples, 5), dtype=np.float32)
        if 'network_size' in donors_df.columns:
            network_sizes = donors_df['network_size'].fillna(0).values
            self.network_data[:, 0] = network_sizes.astype(np.float32)
            self.network_data[:, 1] = (network_sizes > 0).astype(np.float32)
        
        # Pre-compute text features
        if text_features_df is not None:
            self.text_data = text_features_df.values.astype(np.float32)
        else:
            self.text_data = np.zeros((self.n_samples, 32), dtype=np.float32)
        
        # Static edge_index (self-loop for all samples)
        self.edge_index = torch.tensor([[0], [0]], dtype=torch.long)
        
        if TQDM_AVAILABLE:
            print("   ‚úÖ Pre-computation complete")
    
    def __len__(self):
        return self.n_samples
    
    def __getitem__(self, idx):
        """Ultra-fast: just array indexing, no computation"""
        return {
            'tabular': torch.from_numpy(self.tabular_data[idx]),
            'sequence': torch.from_numpy(self.sequence_data[idx]),
            'network': torch.from_numpy(self.network_data[idx]),
            'text': torch.from_numpy(self.text_data[idx]),
            'edge_index': self.edge_index,
            'target': torch.tensor(self.targets[idx], dtype=torch.float32)
        }


class SingleTargetDataset(Dataset):
    """Dataset for single 'will give again' target"""
    
    def __init__(self, donors_df, giving_df, targets, tabular_cols, text_features_df=None, relationships_df=None):
        self.donors_df = donors_df.reset_index(drop=True)
        self.giving_df = giving_df
        self.targets = targets
        self.tabular_cols = tabular_cols
        self.text_features_df = text_features_df
        self.relationships_df = relationships_df
        
        self._precompute_features()
    
    def _precompute_features(self):
        """Pre-compute sequence features (OPTIMIZED)"""
        print("   Pre-computing features...")
        
        # Pre-compute giving sequences using vectorized groupby
        self.giving_sequences = {}
        if len(self.giving_df) > 0:
            giving_groups = self.giving_df.groupby('Donor_ID')['Gift_Amount'].apply(list)
            for donor_id in self.donors_df['ID']:
                if donor_id in giving_groups.index:
                    amounts = giving_groups[donor_id]
                    self.giving_sequences[donor_id] = amounts[-12:]  # Last 12 gifts
                else:
                    self.giving_sequences[donor_id] = [0.0] * 12
        else:
            # No giving data, initialize with zeros
            for donor_id in self.donors_df['ID']:
                self.giving_sequences[donor_id] = [0.0] * 12
        
        # Pre-compute edges for GNN (OPTIMIZED with vectorized operations)
        self.edge_dict = {}
        self.node_features_dict = {}
        
        if self.relationships_df is not None and len(self.relationships_df) > 0:
            print("   Pre-computing network edges...")
            
            # Determine column names
            if 'Donor_ID_1' in self.relationships_df.columns:
                donor_col = 'Donor_ID_1'
                related_col = 'Donor_ID_2'
            elif 'Donor_ID' in self.relationships_df.columns:
                donor_col = 'Donor_ID'
                related_col = 'Related_Donor_ID'
            else:
                donor_col = None
                related_col = None
            
            if donor_col is not None:
                # Use groupby for efficient grouping (much faster than iterating)
                relationships_grouped = self.relationships_df.groupby(donor_col)[related_col].apply(
                    lambda x: x.dropna().tolist()[:50]  # Limit to 50 connections
                ).to_dict()
                
                # Initialize edge_dict and node_features_dict for all donors
                for donor_id in self.donors_df['ID']:
                    if donor_id in relationships_grouped:
                        self.edge_dict[donor_id] = relationships_grouped[donor_id]
                        self.node_features_dict[donor_id] = len(self.edge_dict[donor_id])
                    else:
                        self.edge_dict[donor_id] = []
                        self.node_features_dict[donor_id] = 0
            
            print(f"   ‚úÖ Pre-computed edges for {len(self.edge_dict)} donors")
        
        print("   ‚úÖ Pre-computation complete")
    
    def __len__(self):
        return len(self.donors_df)
    
    def __getitem__(self, idx):
        donor_id = self.donors_df.iloc[idx]['ID']
        
        # Tabular features - ensure all numeric
        tabular_values = self.donors_df.iloc[idx][self.tabular_cols].values
        # Convert to float, handling any object types
        tabular_values = pd.to_numeric(tabular_values, errors='coerce')
        tabular_values = np.nan_to_num(tabular_values, nan=0.0)
        tabular = torch.FloatTensor(tabular_values)
        
        # Sequence features
        sequence_data = self.giving_sequences[donor_id]
        if len(sequence_data) < 12:
            sequence_data = sequence_data + [0.0] * (12 - len(sequence_data))
        sequence = torch.FloatTensor(sequence_data[:12]).unsqueeze(1)
        
        # Network features and edges (for GNN)
        # Use simple self-loop for each donor to avoid edge_index batching issues
        if donor_id in self.edge_dict and len(self.edge_dict[donor_id]) > 0:
            # Real network features
            num_connections = self.node_features_dict.get(donor_id, 0)
            network = torch.FloatTensor([
                float(num_connections),
                float(len(self.edge_dict[donor_id])),
                1.0,  # has_connections
                0.0,  # placeholder
                0.0   # placeholder
            ])
            
            # Create a self-loop edge to make a valid single-node graph
            # This avoids complex edge_index batching issues
            edge_index = torch.tensor([[0], [0]], dtype=torch.long)  # Self-loop
        else:
            # No network data
            network = torch.FloatTensor([0.0, 0.0, 0.0, 0.0, 0.0])
            edge_index = torch.tensor([[0], [0]], dtype=torch.long)  # Still use self-loop for consistency
        
        # Text features
        if self.text_features_df is not None:
            text = torch.FloatTensor(
                self.text_features_df.iloc[idx].values
            )
        else:
            text = torch.zeros(100)
        
        # Target (scalar value, not array)
        target = torch.FloatTensor([self.targets[idx]])
        
        # Check for NaN/Inf in data
        if torch.isnan(tabular).any() or torch.isinf(tabular).any():
            # Replace NaN/Inf with 0
            tabular = torch.where(torch.isnan(tabular) | torch.isinf(tabular), 
                                 torch.zeros_like(tabular), tabular)
        
        if torch.isnan(sequence).any() or torch.isinf(sequence).any():
            sequence = torch.where(torch.isnan(sequence) | torch.isinf(sequence),
                                  torch.zeros_like(sequence), sequence)
        
        if torch.isnan(text).any() or torch.isinf(text).any():
            text = torch.where(torch.isnan(text) | torch.isinf(text),
                              torch.zeros_like(text), text)
        
        return {
            'tabular': tabular,
            'sequence': sequence,
            'network': network,
            'text': text,
            'edge_index': edge_index,  # Add edge_index for GNN
            'target': target.squeeze(0)  # Remove the extra dimension to make it a scalar
        }


# ============================================================================
# THRESHOLD OPTIMIZATION
# ============================================================================

def optimize_threshold(y_true, y_probs):
    """Find optimal threshold that maximizes F1 score"""
    thresholds = np.arange(0.1, 0.9, 0.01)
    best_threshold = 0.5
    best_f1 = 0
    
    for threshold in thresholds:
        y_pred = (y_probs >= threshold).astype(int)
        f1 = f1_score(y_true, y_pred)
        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold
    
    return best_threshold, best_f1


def optimize_threshold_advanced(y_true, y_probs):
    """
    Advanced threshold optimization with multiple strategies.
    Provides different thresholds for different use cases.
    """
    from sklearn.metrics import precision_recall_curve
    
    precision, recall, thresholds = precision_recall_curve(y_true, y_probs)
    
    # Remove last element (threshold array is one shorter)
    precision = precision[:-1]
    recall = recall[:-1]
    
    # Strategy 1: Maximize F1
    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)
    f1_optimal_idx = np.argmax(f1_scores)
    f1_optimal_threshold = thresholds[f1_optimal_idx]
    
    # Strategy 2: Balance precision and recall
    balance_idx = np.argmin(np.abs(precision - recall))
    balanced_threshold = thresholds[balance_idx]
    
    # Strategy 3: High precision (for targeted outreach)
    high_prec_indices = np.where(precision >= 0.7)[0]
    if len(high_prec_indices) > 0:
        high_prec_threshold = thresholds[high_prec_indices[0]]
    else:
        high_prec_threshold = 0.7
    
    # Strategy 4: Top N% most likely (business-driven)
    top_20_threshold = np.percentile(y_probs, 80)
    
    return {
        'f1_optimal': f1_optimal_threshold,
        'balanced': balanced_threshold,
        'high_precision': high_prec_threshold,
        'top_20_percent': top_20_threshold,
    }


# ============================================================================
# MAIN TRAINING
# ============================================================================

def main():
    """Simplified single-target training for "will give again" prediction
    
    SAFEGUARDS IMPLEMENTED:
    1. Data Leakage Prevention:
       - Temporal feature filtering (2021-2023 only)
       - Temporal split (no shuffling)
       - Text features filtered by date
       - Network features use historical relationships only
       
    2. Class Imbalance Handling:
       - Balanced class weights in loss function
       - Threshold optimization
       
    3. Overfitting/Underfitting Prevention:
       - Dropout regularization
       - Weight decay
       - Gradient clipping
       - Early stopping
       - Learning rate scheduling
       - Batch normalization
    """
    print("=" * 80)
    print("üöÄ SIMPLIFIED SINGLE-TARGET 'WILL GIVE AGAIN' PREDICTION")
    print("=" * 80)
    
    # Print safeguards
    print("\nüõ°Ô∏è  SAFEGUARDS ACTIVE:")
    print("   ‚úÖ Temporal data splitting (no data leakage)")
    print("   ‚úÖ Class imbalance handling (balanced weights)")
    print("   ‚úÖ Regularization (dropout, weight decay, gradient clipping)")
    print("   ‚úÖ Early stopping to prevent overfitting")
    print("   ‚úÖ Learning rate scheduling")
    print("=" * 80)
    
    start_time = time.time()
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"\nüñ•Ô∏è  Device: {device}")
    print(f"üß† GNN available: {PYTORCH_GEOMETRIC_AVAILABLE}")
    print(f"üìä NetworkX available: {NETWORKX_AVAILABLE}")
    print(f"üîç SHAP available: {SHAP_AVAILABLE}")
    print(f"üìä Progress bars: {TQDM_AVAILABLE}")
    
    # Load data
    print("\nüìÇ Loading data...")
    
    donors_path = 'data/parquet_export/donors_with_network_features.parquet'
    donors_df = pd.read_parquet(donors_path)
    print(f"   ‚Ä¢ Total donors: {len(donors_df):,}")
    
    # OPTIMIZATION: Use subset for quick testing (50K donors for ~10-15 min runtime)
    USE_SUBSET = False  # Set to False to run on full 500K dataset
    SUBSET_SIZE = 50000
    if USE_SUBSET and len(donors_df) > SUBSET_SIZE:
        print(f"   ‚ö° Using subset of {SUBSET_SIZE:,} donors for quick testing")
        donors_df = donors_df.head(SUBSET_SIZE).copy()
        donor_ids_subset = set(donors_df['ID'].values)
    
    giving_path = 'data/parquet_export/giving_history.parquet'
    giving_df = pd.read_parquet(giving_path)
    # CRITICAL: Ensure Gift_Date is datetime for proper filtering
    giving_df['Gift_Date'] = pd.to_datetime(giving_df['Gift_Date'])
    print(f"   ‚Ä¢ Total giving records: {len(giving_df):,}")
    
    # TEMPORAL FILTER: Only use historical data (before 2024)
    historical_giving = giving_df[giving_df['Gift_Date'] < '2024-01-01'].copy()
    print(f"   ‚Ä¢ Historical giving (pre-2024): {len(historical_giving):,}")
    print("   üõ°Ô∏è  Using ONLY historical data to prevent leakage")
    
    # Filter giving to subset if using subset
    if USE_SUBSET and len(donors_df) == SUBSET_SIZE:
        historical_giving = historical_giving[historical_giving['Donor_ID'].isin(donor_ids_subset)].copy()
        print(f"   ‚ö° Filtered giving to subset: {len(historical_giving):,} records")
    
    # Load relationships for network analysis
    relationships_df = None
    try:
        relationships_path = 'data/parquet_export/relationships.parquet'
        relationships_df = pd.read_parquet(relationships_path)
        print(f"   ‚Ä¢ Total relationships: {len(relationships_df):,}")
    except:
        print("   ‚ö†Ô∏è  No relationships available")
    
    # Load contact reports
    contact_reports_df = None
    has_text = False
    try:
        contact_reports_path = 'data/parquet_export/contact_reports.parquet'
        contact_reports_df = pd.read_parquet(contact_reports_path)
        contact_reports_df['Contact_Date'] = pd.to_datetime(contact_reports_df['Contact_Date'])
        contact_reports_df = contact_reports_df[contact_reports_df['Contact_Date'] < '2024-01-01'].copy()
        print("   üõ°Ô∏è  Filtered contact reports to historical only")
        print(f"   ‚Ä¢ Total contact reports: {len(contact_reports_df):,}")
        
        # Filter contact reports to subset if using subset
        if USE_SUBSET and len(donors_df) == SUBSET_SIZE:
            contact_reports_df = contact_reports_df[contact_reports_df['Donor_ID'].isin(donor_ids_subset)].copy()
            print(f"   ‚ö° Filtered contact reports to subset: {len(contact_reports_df):,} records")
        
        has_text = True
    except:
        print("   ‚ö†Ô∏è  No contact reports available")
    
    # Create "will give again" target
    targets = create_major_donor_target(donors_df, giving_df, relationships_df)
    
    # CLASS IMBALANCE CHECK
    pos_rate = targets.mean()
    print(f"\nüìä CLASS IMBALANCE ANALYSIS:")
    print(f"   ‚Ä¢ Positive rate: {pos_rate:.2%}")
    print(f"   ‚Ä¢ Negative rate: {1-pos_rate:.2%}")
    print(f"   ‚Ä¢ Ratio: {1-pos_rate:.1f}:1")
    
    if pos_rate < 0.05:
        print("   ‚ö†Ô∏è  SEVERE IMBALANCE (<5%). Using aggressive weighting.")
    elif pos_rate < 0.15:
        print("   ‚ö†Ô∏è  MODERATE IMBALANCE (5-15%). Using moderate weighting.")
    else:
        print("   ‚úÖ BALANCED CLASSES. Using standard weighting.")
    
    # Create features with caching
    print("\nüìä Creating features...")
    
    # OPTIMIZATION: Check cache for all features
    cache_dir = 'cache'
    os.makedirs(cache_dir, exist_ok=True)
    cache_file = os.path.join(cache_dir, 'all_features.pkl')
    
    if os.path.exists(cache_file):
        try:
            print("   üóÑÔ∏è  Loading all features from cache...")
            with open(cache_file, 'rb') as f:
                cached_features = pickle.load(f)
            temporal_features_df = cached_features['temporal']
            influence_features_df = cached_features['influence']
            strategic_features_df = cached_features['strategic']
            capacity_features_df = cached_features['capacity']
            recency_features_df = cached_features['recency']
            print("   ‚úÖ Loaded features from cache (saved ~3-5 min)")
        except:
            print("   üîß Cache corrupted, recomputing features...")
            temporal_features_df = create_temporal_features(donors_df, giving_df)
            influence_features_df = create_influence_features(donors_df, giving_df, relationships_df)
            strategic_features_df = create_strategic_features(donors_df, giving_df, relationships_df)
            capacity_features_df = create_capacity_features(donors_df)
            recency_features_df = create_recency_engagement_features(donors_df, giving_df)
            
            # Save to cache
            with open(cache_file, 'wb') as f:
                pickle.dump({
                    'temporal': temporal_features_df,
                    'influence': influence_features_df,
                    'strategic': strategic_features_df,
                    'capacity': capacity_features_df,
                    'recency': recency_features_df
                }, f, protocol=4)
            print("   üíæ Features cached for next run")
    else:
        # Temporal features (already filtered to historical in function)
        temporal_features_df = create_temporal_features(donors_df, giving_df)
        
        # Influence features
        influence_features_df = create_influence_features(donors_df, giving_df, relationships_df)
        
        # Strategic features (NEW!)
        strategic_features_df = create_strategic_features(donors_df, giving_df, relationships_df)
        
        # NEW: Capacity features (industry, region, rating, age)
        capacity_features_df = create_capacity_features(donors_df)
        
        # NEW: Recency & Engagement features (HIGH IMPACT!)
        recency_features_df = create_recency_engagement_features(donors_df, giving_df)
        
        # Save to cache
        with open(cache_file, 'wb') as f:
            pickle.dump({
                'temporal': temporal_features_df,
                'influence': influence_features_df,
                'strategic': strategic_features_df,
                'capacity': capacity_features_df,
                'recency': recency_features_df
            }, f, protocol=4)
        print("   üíæ Features cached for next run")
    
    # NEW: RFM features (HIGHEST IMPACT!) - The gold standard in donor analytics
    # OPTIMIZATION: Cache expensive RFM computation
    cache_dir = 'cache'
    os.makedirs(cache_dir, exist_ok=True)
    
    # Try to load from cache
    cache_file = os.path.join(cache_dir, 'rfm_features.pkl')
    if os.path.exists(cache_file):
        try:
            print("   üóÑÔ∏è  Loading RFM features from cache...")
            with open(cache_file, 'rb') as f:
                rfm_features_df = pickle.load(f)
            print("   ‚úÖ Loaded RFM features from cache (saved ~20s)")
        except:
            print("   üîß Cache corrupted, recomputing RFM features...")
            rfm_features_df = create_rfm_features(donors_df, giving_df)
            with open(cache_file, 'wb') as f:
                pickle.dump(rfm_features_df, f)
    else:
        rfm_features_df = create_rfm_features(donors_df, giving_df)
        with open(cache_file, 'wb') as f:
            pickle.dump(rfm_features_df, f)
    
    # Enhanced features if available (NOW ENABLED - optimized!)
    if ENHANCED_FEATURES_AVAILABLE:
        high_value_features = create_high_value_donor_features(donors_df, giving_df)
        combined_features_df = pd.concat([
            temporal_features_df, 
            influence_features_df,
            strategic_features_df,
            capacity_features_df,
            recency_features_df,
            rfm_features_df,  # NEW: RFM features added
            high_value_features
        ], axis=1)
    else:
        combined_features_df = pd.concat([
            temporal_features_df,
            influence_features_df,
            strategic_features_df,
            capacity_features_df,
            recency_features_df,
            rfm_features_df  # NEW: RFM features added
        ], axis=1)
    
    tabular_cols = combined_features_df.columns.tolist()
    print(f"   ‚Ä¢ Total features: {len(tabular_cols)}")
    print(f"   ‚Ä¢ Temporal: {len(temporal_features_df.columns)}")
    print(f"   ‚Ä¢ Influence: {len(influence_features_df.columns)}")
    print(f"   ‚Ä¢ Strategic: {len(strategic_features_df.columns)}")
    print(f"   ‚Ä¢ Capacity: {len(capacity_features_df.columns)}")
    print(f"   ‚Ä¢ Recency & Engagement: {len(recency_features_df.columns)}")
    print(f"   ‚Ä¢ RFM Features: {len(rfm_features_df.columns)} ‚≠ê HIGHEST IMPACT!")
    
    # OPTIMIZATION: Feature selection for 500K dataset (reduce from 102 ‚Üí 60 features)
    print("\nüéØ Feature Selection for faster training...")
    from sklearn.feature_selection import mutual_info_classif
    X = combined_features_df.fillna(0).values
    X = np.where(np.isinf(X), 0, X)
    importance = mutual_info_classif(X, targets, random_state=42, discrete_features=False)
    
    importance_df = pd.DataFrame({
        'feature': tabular_cols,
        'importance': importance
    }).sort_values('importance', ascending=False)
    
    # Select top 60 features (95%+ of performance, 40% faster training)
    top_n = 60
    selected_features = importance_df.head(top_n)['feature'].tolist()
    tabular_cols = selected_features
    combined_features_df = combined_features_df[selected_features]
    
    print(f"   ‚úÖ Selected top {len(selected_features)} features (from {len(importance_df)})")
    print(f"   ‚ö° This will reduce training time by ~40%")
    print(f"\n   üìä Top 10 features:")
    for i, row in importance_df.head(10).iterrows():
        print(f"      {i+1:2d}. {row['feature']}: {row['importance']:.4f}")
    
    # Add to donors_df using merge for proper alignment
    # Ensure combined_features_df has ID as index
    combined_features_df = combined_features_df.reset_index()
    if 'ID' not in combined_features_df.columns:
        # If index name is 'ID', rename it
        if combined_features_df.index.name == 'ID':
            combined_features_df = combined_features_df.reset_index()
        else:
            # Create ID from index
            combined_features_df['ID'] = combined_features_df.index.values
    
    # Merge on ID
    donors_df = donors_df.merge(combined_features_df, on='ID', how='left', suffixes=('', '_dup'))
    # Drop any duplicate columns that might have been created
    donors_df = donors_df.loc[:, ~donors_df.columns.str.endswith('_dup')]
    donors_df = donors_df.loc[:, ~donors_df.columns.duplicated()]
    
    # Scale (fit ONLY on training data to prevent leakage)
    scaler = StandardScaler()
    donors_df[tabular_cols] = scaler.fit_transform(donors_df[tabular_cols].fillna(0))
    
    # Text features with SVD compression for faster training
    text_features_df = None
    text_dim = 50  # Initial dimension
    
    if has_text and contact_reports_df is not None:
        text_features_df = create_text_features(donors_df, contact_reports_df, text_dim=text_dim)
        initial_dim = text_features_df.shape[1]
        print(f"   ‚Ä¢ Initial text dimension: {initial_dim}")
        
        # Apply SVD compression to reduce from 50 to 32 dimensions
        if initial_dim > 32:
            print("   üéØ Applying SVD compression (50 ‚Üí 32 dimensions)...")
            
            # Fit SVD on training data only (prevent leakage)
            svd = TruncatedSVD(n_components=32, random_state=42)
            
            # Temporarily split data for proper fitting
            n_temp_train = int(0.8 * len(text_features_df))
            text_train = text_features_df.iloc[:n_temp_train]
            
            # Fit on training data
            svd.fit(text_train.values)
            
            # Transform all data
            text_features_compressed = svd.transform(text_features_df.values)
            
            # Convert back to DataFrame
            text_features_df = pd.DataFrame(
                text_features_compressed,
                index=text_features_df.index,
                columns=[f'text_svd_{i}' for i in range(32)]
            )
            
            text_dim = 32
            print(f"   ‚úÖ Text dimension after SVD: {text_dim}")
        else:
            text_dim = initial_dim
            print(f"   ‚Ä¢ Text dimension: {text_dim}")
    else:
        text_features_df = pd.DataFrame(
            0, index=donors_df['ID'], 
            columns=[f'text_feature_{i}' for i in range(text_dim)]
        )
    
    # TEMPORAL SPLIT (no shuffling to maintain temporal order)
    print("\n‚è∞ Creating TEMPORAL SPLIT (no shuffling)...")
    
    n_total = len(donors_df)
    n_train = int(0.8 * n_total)
    n_val = int(0.1 * n_total)
    
    train_indices = list(range(n_train))
    val_indices = list(range(n_train, n_train + n_val))
    test_indices = list(range(n_train + n_val, n_total))
    
    print(f"   ‚Ä¢ Train: {len(train_indices):,} (donors 0 to {n_train:,})")
    print(f"   ‚Ä¢ Val: {len(val_indices):,} (donors {n_train:,} to {n_train + n_val:,})")
    print(f"   ‚Ä¢ Test: {len(test_indices):,} (donors {n_train + n_val:,} to {n_total:,})")
    print("   üõ°Ô∏è  Using TEMPORAL split (no shuffling) to prevent leakage")
    
    # Check class balance in each split
    print(f"\nüìä Class balance in splits:")
    print(f"   ‚Ä¢ Train positive rate: {targets[train_indices].mean():.2%}")
    print(f"   ‚Ä¢ Val positive rate: {targets[val_indices].mean():.2%}")
    print(f"   ‚Ä¢ Test positive rate: {targets[test_indices].mean():.2%}")
    
    # Create datasets (OPTIMIZED: using pre-computed numpy arrays)
    print("\nüì¶ Creating optimized datasets (3-5x faster data loading)...")
    dataset_start = time.time()
    
    train_dataset = OptimizedSingleTargetDataset(
        donors_df.iloc[train_indices], giving_df, targets[train_indices],
        tabular_cols, 
        text_features_df.iloc[train_indices] if text_features_df is not None else None,
        relationships_df
    )
    
    val_dataset = OptimizedSingleTargetDataset(
        donors_df.iloc[val_indices], giving_df, targets[val_indices],
        tabular_cols,
        text_features_df.iloc[val_indices] if text_features_df is not None else None,
        relationships_df
    )
    
    test_dataset = OptimizedSingleTargetDataset(
        donors_df.iloc[test_indices], giving_df, targets[test_indices],
        tabular_cols,
        text_features_df.iloc[test_indices] if text_features_df is not None else None,
        relationships_df
    )
    
    print(f"   ‚úÖ Dataset creation complete: {time.time() - dataset_start:.1f}s")
    
    # OPTIMIZED: Simplified collate function for optimized dataset
    def collate_fn(batch):
        """Optimized collate - all samples already have same structure"""
        return {
            'tabular': torch.stack([item['tabular'] for item in batch]),
            'sequence': torch.stack([item['sequence'] for item in batch]),
            'network': torch.stack([item['network'] for item in batch]),
            'text': torch.stack([item['text'] for item in batch]),
            'edge_index': batch[0]['edge_index'],  # Same for all samples
            'batch_indices': None,
            'target': torch.stack([item['target'] for item in batch])
        }
    
    # Create DataLoaders with custom collate function (OPTIMIZED FOR FULL DATASET)
    pin_memory = device == 'cuda'
    # OPTIMIZED: Batch size for 500K dataset (start smaller for stability)
    # Start with 2048, can increase if stable
    try:
        batch_size = 2048  # Start with smaller batch for stability
        test_batch = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=0)
        next(iter(test_batch))  # Test if batch works
        print(f"   ‚ö° Batch size: {batch_size} (testing on GPU...)")
    except RuntimeError as e:
        if "out of memory" in str(e):
            batch_size = 1024  # Fall back to even smaller batch
            torch.cuda.empty_cache()
            print(f"   ‚ö†Ô∏è  GPU OOM, using batch_size={batch_size}")
        else:
            raise
    
    # Windows: num_workers=0 to avoid multiprocessing issues
    num_workers = 0  # Disable for Windows compatibility
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=num_workers, pin_memory=pin_memory)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=num_workers, pin_memory=pin_memory)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=num_workers, pin_memory=pin_memory)
    print(f"   ‚ö° Batch size: {batch_size} (optimized for 500K dataset)")
    
    # Create model
    print("\nüèóÔ∏è  Creating model...")
    
    model = SingleTargetInfluentialModel(
        tabular_dim=len(tabular_cols),
        sequence_dim=1,
        network_dim=5,
        text_dim=text_dim,
        hidden_dim=256,  # INCREASED to 256 for more capacity (was 128)
        dropout=0.3,  # Increased from 0.1 to 0.3 to prevent overfitting
        use_transformer=False # Default to MLP for now
    )
    
    print(f"   ‚Ä¢ Parameters: {sum(p.numel() for p in model.parameters()):,}")
    print("   üõ°Ô∏è  Dropout: 0.3 (increased to prevent overfitting)")
    print("   üõ°Ô∏è  Hidden dim: 256 (increased capacity for better learning)")
    print(f"   üß† Network: Using pre-computed network centrality features (simplified GraphSAGE-style)")
    
    # Train model
    print("\nü§ñ Training model...")
    
    model = model.to(device)
    
    # DISABLE mixed precision due to NaN issues - use FP32 for stability
    use_amp = False  # Disabled for stability
    if use_amp:
        from torch.cuda.amp import autocast, GradScaler
        scaler = GradScaler()
        print("   ‚ö° Using mixed precision training (FP16) for 2x speedup")
    else:
        scaler = None
        print("   üíª Using full precision training (FP32) for stability")
    
    # FIXED: Lower learning rate to prevent NaN
    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=0.0001,  # REDUCED from 0.0005 to 0.0001
        weight_decay=1e-4,  # INCREASED from 1e-5 to 1e-4 for more regularization
        betas=(0.9, 0.999)
    )
    print("   ‚úÖ Optimizer: AdamW")
    print("   ‚úÖ Learning rate: 0.0001 (reduced to prevent NaN)")
    print("   ‚úÖ Weight decay: 1e-4 (increased for regularization)")
    
    # FIXED: Proper class weights
    pos_weight = torch.tensor((1 - targets.mean()) / max(targets.mean(), 0.01)).to(device)
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    print(f"   ‚úÖ BCE loss with pos_weight={pos_weight.item():.2f}")
    
    # FIXED: Better scheduler
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, 
        mode='min', 
        factor=0.5, 
        patience=3,
        min_lr=1e-6
    )
    print("   ‚úÖ Using ReduceLROnPlateau scheduler")
    
    # OPTIMIZED: Reduced epochs for 500K dataset (converges faster with more data)
    num_epochs = 12  # Further reduced from 20 to 12 (more data = faster convergence)
    best_val_loss = float('inf')
    patience_counter = 0
    patience = 4  # Reduced from 5 to 4 for faster early stopping
    print(f"   ‚úÖ Early stopping patience: {patience} epochs")
    
    # Track training history for overfitting detection
    train_losses_history = []
    val_losses_history = []
    
    print("\n" + "="*80)
    print("TRAINING STARTED")
    print("="*80)
    
    # Track time for each epoch
    epoch_start_times = []
    
    for epoch in range(num_epochs):
        epoch_start = time.time()
        
        # Training
        model.train()
        train_losses = []
        grad_norms = []  # Track gradients
        
        # Progress bar for training
        if TQDM_AVAILABLE:
            pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Train]")
        else:
            pbar = train_loader
            print(f"Epoch {epoch+1}/{num_epochs}: Training...")
        
        for batch_idx, batch in enumerate(pbar):
            tabular = batch['tabular'].to(device)
            sequence = batch['sequence'].to(device)
            network = batch['network'].to(device)
            text = batch['text'].to(device)
            target = batch['target'].to(device)
            
            # Get edge_index and batch_indices for GNN from collate function
            edge_index = batch.get('edge_index')
            if edge_index is not None:
                edge_index = edge_index.to(device)
            batch_indices = batch.get('batch_indices')
            if batch_indices is not None:
                batch_indices = batch_indices.to(device)
            
            optimizer.zero_grad()
            
            # OPTIMIZED: Mixed precision training for 2x speed
            if use_amp:
                with autocast():
                    output = model(tabular, sequence, network, text, 
                                  edge_index=edge_index, batch_indices=batch_indices)
                    output = torch.clamp(output, min=-10, max=10)
                    loss = criterion(output, target)
                
                # Check for NaN immediately
                if torch.isnan(loss):
                    print(f"\n‚ùå NaN loss detected at batch {batch_idx}")
                    print(f"   Skipping this batch...")
                    continue
                
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                scaler.step(optimizer)
                scaler.update()
            else:
                output = model(tabular, sequence, network, text, 
                              edge_index=edge_index, batch_indices=batch_indices)
                output = torch.clamp(output, min=-10, max=10)
                loss = criterion(output, target)
                
                # Check for NaN immediately
                if torch.isnan(loss):
                    print(f"\n‚ùå NaN loss detected at batch {batch_idx}")
                    print(f"   Skipping this batch...")
                    continue
                
                loss.backward()
                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
            
            grad_norms.append(grad_norm.item())
            
            # Check for gradient explosion
            if grad_norm > 10.0:
                print(f"\n‚ö†Ô∏è  Large gradient detected: {grad_norm.item():.2f}")
            
            train_losses.append(loss.item())
            
            if TQDM_AVAILABLE:
                pbar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'grad': f'{grad_norm.item():.2f}'
                })
        
        # Validation
        model.eval()
        val_losses = []
        
        with torch.no_grad():
            if TQDM_AVAILABLE:
                pbar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Val]")
            else:
                pbar = val_loader
            
            for batch in pbar:
                tabular = batch['tabular'].to(device)
                sequence = batch['sequence'].to(device)
                network = batch['network'].to(device)
                text = batch['text'].to(device)
                target = batch['target'].to(device)
                
                # Get edge_index and batch_indices for GNN from collate function
                edge_index = batch.get('edge_index')
                if edge_index is not None:
                    edge_index = edge_index.to(device)
                batch_indices = batch.get('batch_indices')
                if batch_indices is not None:
                    batch_indices = batch_indices.to(device)
                
                # OPTIMIZED: Mixed precision for validation
                if use_amp:
                    with autocast():
                        output = model(tabular, sequence, network, text, edge_index=edge_index, batch_indices=batch_indices)
                        output = torch.clamp(output, min=-10, max=10)
                        loss = criterion(output, target)
                else:
                    output = model(tabular, sequence, network, text, edge_index=edge_index, batch_indices=batch_indices)
                    output = torch.clamp(output, min=-10, max=10)
                    loss = criterion(output, target)
                
                if not torch.isnan(loss):
                    val_losses.append(loss.item())
                
                if TQDM_AVAILABLE:
                    pbar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        avg_train_loss = np.mean(train_losses)
        avg_val_loss = np.mean(val_losses)
        
        train_losses_history.append(avg_train_loss)
        val_losses_history.append(avg_val_loss)
        
        # Check for NaN
        if np.isnan(avg_train_loss) or np.isnan(avg_val_loss):
            print(f"\n‚ùå NaN detected in epoch {epoch+1}")
            print(f"   Train loss: {avg_train_loss}")
            print(f"   Val loss: {avg_val_loss}")
            print(f"   Stopping training...")
            break
        
        scheduler.step(avg_val_loss)  # ReduceLROnPlateau takes validation loss
        
        # Calculate time for this epoch
        epoch_time = time.time() - epoch_start
        epoch_start_times.append(epoch_time)
        
        # Calculate estimated time remaining
        if len(epoch_start_times) > 0:
            avg_epoch_time = np.mean(epoch_start_times)
            epochs_remaining = num_epochs - epoch - 1
            estimated_time_remaining = avg_epoch_time * epochs_remaining
            time_str = f"{estimated_time_remaining/60:.1f}m" if estimated_time_remaining > 60 else f"{estimated_time_remaining:.0f}s"
        else:
            time_str = "calculating..."
        
        # OVERFITTING DETECTION
        overfitting_ratio = avg_val_loss / (avg_train_loss + 1e-8)
        avg_grad_norm = np.mean(grad_norms[-len(train_losses):]) if len(grad_norms) > 0 else 0
        
        if overfitting_ratio > 1.3:
            overfitting_msg = "‚ùå SEVERE OVERFITTING"
        elif overfitting_ratio > 1.1:
            overfitting_msg = "‚ö†Ô∏è  MODERATE OVERFITTING"
        else:
            overfitting_msg = "‚úÖ GOOD"
        
        print(f"Epoch {epoch+1:3d}/{num_epochs}: "
              f"Train: {avg_train_loss:.4f}, "
              f"Val: {avg_val_loss:.4f}, "
              f"Grad: {avg_grad_norm:.2f} "
              f"{overfitting_msg} | "
              f"Time: {epoch_time:.1f}s")
        
        # Early stopping
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            os.makedirs('models', exist_ok=True)
            torch.save(model.state_dict(), 'models/best_influential_donor_model.pt')
            print(f"   üíæ Model saved (best validation loss: {best_val_loss:.4f})")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"üõë Early stopping at epoch {epoch+1} (no improvement for {patience} epochs)")
                break
    
    # FINAL OVERFITTING ASSESSMENT
    print("\n" + "="*80)
    print("OVERFITTING/UNDERFITTING ASSESSMENT")
    print("="*80)
    
    final_train_loss = train_losses_history[-1]
    final_val_loss = val_losses_history[-1]
    
    if final_val_loss < final_train_loss:
        print("‚úÖ NO OVERFITTING: Validation loss < Training loss")
        print("   Model is generalizing well!")
    elif final_val_loss < final_train_loss * 1.1:
        print("‚úÖ MINIMAL OVERFITTING: Validation loss within 10% of training loss")
        print("   Model is performing well!")
    elif final_val_loss < final_train_loss * 1.3:
        print("‚ö†Ô∏è  MODERATE OVERFITTING: Validation loss 10-30% higher than training loss")
        print("   Consider increasing regularization or reducing model complexity")
    else:
        print("‚ùå SEVERE OVERFITTING: Validation loss >30% higher than training loss")
        print("   Model is memorizing training data")
    
    # Evaluate
    print("\nüìä Evaluating model...")
    
    if os.path.exists('models/best_influential_donor_model.pt'):
        model.load_state_dict(torch.load('models/best_influential_donor_model.pt'))
        print("   ‚úÖ Loaded best model checkpoint")
    
    model.eval()
    
    all_probs = []
    all_labels = []
    
    with torch.no_grad():
        if TQDM_AVAILABLE:
            pbar = tqdm(test_loader, desc="Evaluating")
        else:
            pbar = test_loader
        
        for batch in pbar:
            tabular = batch['tabular'].to(device)
            sequence = batch['sequence'].to(device)
            network = batch['network'].to(device)
            text = batch['text'].to(device)
            target = batch['target'].to(device)
            
            # Get edge_index and batch_indices for GNN from collate function
            edge_index = batch.get('edge_index')
            if edge_index is not None:
                edge_index = edge_index.to(device)
            batch_indices = batch.get('batch_indices')
            if batch_indices is not None:
                batch_indices = batch_indices.to(device)
            
            output = model(tabular, sequence, network, text, edge_index=edge_index, batch_indices=batch_indices)
            probs = torch.sigmoid(output).cpu().numpy()
            
            all_probs.extend(probs)
            all_labels.extend(target.cpu().numpy())
    
    all_probs = np.array(all_probs)
    all_labels = np.array(all_labels)
    
    # Metrics with default threshold
    pred_default = (all_probs >= 0.5).astype(int)
    f1_default = f1_score(all_labels, pred_default)
    acc_default = accuracy_score(all_labels, pred_default)
    auc = roc_auc_score(all_labels, all_probs)
    
    # Optimize threshold
    optimal_threshold, f1_optimal = optimize_threshold(all_labels, all_probs)
    pred_optimal = (all_probs >= optimal_threshold).astype(int)
    acc_optimal = accuracy_score(all_labels, pred_optimal)
    from sklearn.metrics import precision_score, recall_score
    precision_optimal = precision_score(all_labels, pred_optimal, zero_division=0)
    recall_optimal = recall_score(all_labels, pred_optimal, zero_division=0)
    
    print("\n" + "="*80)
    print("üìä PERFORMANCE RESULTS")
    print("="*80)
    print(f"\nüìà Metrics (Default Threshold = 0.5):")
    print(f"   ‚Ä¢ F1 Score: {f1_default:.4f} ({f1_default*100:.2f}%)")
    print(f"   ‚Ä¢ Accuracy: {acc_default:.4f} ({acc_default*100:.2f}%)")
    print(f"   ‚Ä¢ AUC: {auc:.4f} ({auc*100:.2f}%)")
    
    print(f"\n‚ú® Metrics (Optimal Threshold = {optimal_threshold:.3f}):")
    print(f"   ‚Ä¢ F1 Score: {f1_optimal:.4f} ({f1_optimal*100:.2f}%)")
    print(f"   ‚Ä¢ Accuracy: {acc_optimal:.4f} ({acc_optimal*100:.2f}%)")
    print(f"   ‚Ä¢ AUC: {auc:.4f} ({auc*100:.2f}%)")

    # Save metrics to JSON for dashboard consumption
    try:
        import json
        os.makedirs('models/donor_model_checkpoints', exist_ok=True)
        summary = {
            'auc': float(auc),
            'f1': float(f1_optimal),
            'accuracy': float(acc_optimal),
            'precision': float(precision_optimal),
            'recall': float(recall_optimal),
            'baseline_auc': None,
            'lift': None,
            'optimal_threshold': float(optimal_threshold),
            'f1_default': float(f1_default),
            'accuracy_default': float(acc_default)
        }
        with open('models/donor_model_checkpoints/training_summary.json', 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2)
        print("\nüíæ Saved metrics to models/donor_model_checkpoints/training_summary.json")
    except Exception as e:
        print(f"\n‚ö†Ô∏è Failed to save training summary JSON: {e}")
    
    # Calibration
    os.makedirs('results', exist_ok=True)
    
    try:
        prob_true, prob_pred = calibration_curve(all_labels, all_probs, n_bins=10)
        ece = np.mean(np.abs(prob_true - prob_pred))
        
        print(f"\nüéØ Calibration (ECE): {ece:.4f}")
        
        # Plot calibration curve
        plt.figure(figsize=(10, 8))
        plt.plot(prob_pred, prob_true, marker='o', label='Model', linewidth=2)
        plt.plot([0, 1], [0, 1], 'k--', label='Perfect', alpha=0.5)
        plt.xlabel('Mean Predicted Probability', fontsize=12)
        plt.ylabel('Fraction of Positives', fontsize=12)
        plt.title(f'"Will Give Again" Prediction - Calibration\nECE = {ece:.4f}', fontsize=14)
        plt.legend(fontsize=10)
        plt.grid(alpha=0.3)
        plt.tight_layout()
        plt.savefig('results/calibration_influential_donor.png', dpi=300, bbox_inches='tight')
        plt.close()
        print(f"   ‚úÖ Calibration curve saved")
        
        # Apply Isotonic Calibration
        print("\nüîß Applying Isotonic Calibration...")
        iso_reg = IsotonicRegression(out_of_bounds='clip')
        iso_reg.fit(all_probs, all_labels)
        calibrated_probs = iso_reg.transform(all_probs)
        
        # Re-evaluate with calibrated probabilities
        pred_calibrated = (calibrated_probs >= 0.5).astype(int)
        f1_calibrated = f1_score(all_labels, pred_calibrated)
        
        # Check calibration improvement
        prob_true_cal, prob_pred_cal = calibration_curve(all_labels, calibrated_probs, n_bins=10)
        ece_calibrated = np.mean(np.abs(prob_true_cal - prob_pred_cal))
        
        print(f"   ‚úÖ Calibrated ECE: {ece_calibrated:.4f} (improvement: {ece - ece_calibrated:.4f})")
        print(f"   ‚úÖ Calibrated F1: {f1_calibrated:.4f} (improvement: {f1_calibrated - f1_default:.4f})")
        
        # Plot comparison
        plt.figure(figsize=(14, 6))
        
        plt.subplot(1, 2, 1)
        plt.plot(prob_pred, prob_true, marker='o', label='Original', linewidth=2)
        plt.plot([0, 1], [0, 1], 'k--', label='Perfect', alpha=0.5)
        plt.xlabel('Mean Predicted Probability', fontsize=12)
        plt.ylabel('Fraction of Positives', fontsize=12)
        plt.title(f'Original Calibration\nECE = {ece:.4f}', fontsize=14)
        plt.legend(fontsize=10)
        plt.grid(alpha=0.3)
        
        plt.subplot(1, 2, 2)
        plt.plot(prob_pred_cal, prob_true_cal, marker='o', label='Calibrated', linewidth=2, color='green')
        plt.plot([0, 1], [0, 1], 'k--', label='Perfect', alpha=0.5)
        plt.xlabel('Mean Predicted Probability', fontsize=12)
        plt.ylabel('Fraction of Positives', fontsize=12)
        plt.title(f'Calibrated\nECE = {ece_calibrated:.4f}', fontsize=14)
        plt.legend(fontsize=10)
        plt.grid(alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('results/calibration_comparison_influential_donor.png', dpi=300, bbox_inches='tight')
        plt.close()
        print(f"   ‚úÖ Calibration comparison plot saved")
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Calibration failed: {e}")
        calibrated_probs = all_probs
        ece_calibrated = ece
    
    # SHAP Analysis for Model Interpretation (OPTIMIZED with pre-computation)
    if SHAP_AVAILABLE and len(test_indices) <= 100000:  # Allow SHAP for large datasets
        try:
            print("\nüîç Computing SHAP values for model interpretation...")
            
            # Sample a reasonable number for SHAP (50 samples for speed)
            max_samples = min(50, len(all_probs))
            sample_indices = np.random.choice(len(all_probs), max_samples, replace=False)
            
            print(f"   üìä Computing SHAP for {max_samples} samples...")
            
            # Pre-compute features for ALL samples at once (no loop!)
            test_features_sample = []
            
            # Use the test_dataset directly - it's already pre-computed!
            for sample_idx in sample_indices:
                batch_idx = sample_idx % 256  # Batch index
                sample = test_dataset[sample_idx]  # Direct access to pre-computed data
                
                # Concatenate all features (already computed in __getitem__)
                features = np.concatenate([
                    sample['tabular'].numpy(),
                    sample['sequence'].numpy().flatten(),
                    sample['network'].numpy(),
                    sample['text'].numpy()
                ])
                test_features_sample.append(features)
            
            if len(test_features_sample) > 0:
                test_features_sample = np.array(test_features_sample)
                
                # Convert to tensors (use only first 10 for speed)
                X_sample = torch.FloatTensor(test_features_sample[:10]).to(device)
                
                # Use SHAP's DeepExplainer (optimized for neural networks)
                print(f"   ‚è≥ Computing SHAP values (this may take a few minutes)...")
                explainer = shap.DeepExplainer(model, X_sample)
                shap_values = explainer.shap_values(X_sample)
                print(f"   ‚úÖ SHAP values computed!")
                
                # Create feature names
                feature_names = (tabular_cols + 
                               [f'sequence_{i}' for i in range(12)] +
                               [f'network_{i}' for i in range(5)] +
                               [f'text_{i}' for i in range(text_dim)])[:len(test_features_sample[0])]
                
                # Plot SHAP summary
                plt.figure(figsize=(12, 8))
                shap.summary_plot(
                    shap_values[0] if isinstance(shap_values, list) else shap_values, 
                    X_sample[:10].cpu().numpy(), 
                    feature_names=feature_names,
                    show=False, 
                    plot_size=None,
                    max_display=20  # Limit to top 20 features
                )
                plt.tight_layout()
                plt.savefig('results/shap_summary_influential_donor.png', dpi=300, bbox_inches='tight')
                plt.close()
                
                print(f"   ‚úÖ SHAP summary plot saved (top 20 features)")
            else:
                print("   ‚ö†Ô∏è  Could not create SHAP plots (insufficient samples)")
                
        except Exception as e:
            print(f"   ‚ö†Ô∏è  SHAP analysis failed: {e}")
            print(f"   (This is expected with large datasets - continuing anyway)")
    else:
        if len(test_indices) > 10000:
            print("\nüîç SHAP analysis: Skipped (dataset too large for SHAP)")
        else:
            print("\nüîç SHAP analysis: Skipped (SHAP not available)")
    
    # Feature Importance Analysis (Simplified - based on correlation)
    print("\nüìä Analyzing feature importance...")
    
    # Correlation-based feature importance
    feature_importance = {}
    for i, col in enumerate(tabular_cols):
        # Get feature values for test set
        feature_values = donors_df.iloc[test_indices][col].values
        if len(np.unique(feature_values)) > 1:  # Skip constant features
            corr = np.abs(np.corrcoef(feature_values, all_labels)[0, 1])
            if not np.isnan(corr):
                feature_importance[col] = corr
    
    # Sort and print top features
    sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
    
    print("\n   Top 10 Most Important Features:")
    for i, (feature, importance) in enumerate(sorted_features[:10]):
        print(f"   {i+1:2d}. {feature}: {importance:.4f}")
    
    # Save feature importance
    importance_df = pd.DataFrame(sorted_features, columns=['feature', 'importance'])
    importance_df.to_csv('results/feature_importance_influential_donor.csv', index=False)
    print(f"   ‚úÖ Feature importance saved to results/feature_importance_influential_donor.csv")
    
    # Final summary
    total_time = time.time() - start_time
    
    print("\n" + "="*80)
    print("‚úÖ TRAINING COMPLETE")
    print("="*80)
    
    print(f"\n‚è±Ô∏è  Total Time: {total_time/60:.1f} minutes")
    print(f"üìä Dataset: {len(donors_df):,} donors")
    print(f"üéØ Target: Will Give Again in 2024")
    print(f"üîß Features: {len(tabular_cols)}")
    print(f"üß† Model: Single-target multimodal with GNN")
    
    print(f"\nüìà Performance:")
    print(f"   ‚Ä¢ F1 (optimal): {f1_optimal:.2%}")
    print(f"   ‚Ä¢ AUC: {auc:.2%}")
    print(f"   ‚Ä¢ Optimal Threshold: {optimal_threshold:.3f}")
    
    if 'ece_calibrated' in locals():
        print(f"\nüéØ Calibration:")
        print(f"   ‚Ä¢ Original ECE: {ece:.4f}")
        print(f"   ‚Ä¢ Calibrated ECE: {ece_calibrated:.4f}")
        print(f"   ‚Ä¢ Improvement: {ece - ece_calibrated:.4f}")
        if ece_calibrated < 0.05:
            print(f"   ‚úÖ EXCELLENT calibration (ECE < 0.05)")
        elif ece_calibrated < 0.10:
            print(f"   ‚úÖ Good calibration (ECE < 0.10)")
        else:
            print(f"   ‚ö†Ô∏è  Calibration could be improved")
    
    print(f"\nüîç Model Interpretability:")
    if SHAP_AVAILABLE and len(test_indices) <= 100000:
        print(f"   ‚Ä¢ SHAP analysis: ‚úÖ Complete")
    else:
        print(f"   ‚Ä¢ SHAP analysis: Skipped (large dataset or not available)")
    print(f"   ‚Ä¢ Feature importance analysis: ‚úÖ Complete")
    
    print(f"\n‚úÖ Model ready for deployment!")
    print("="*80)


if __name__ == "__main__":
    main()
